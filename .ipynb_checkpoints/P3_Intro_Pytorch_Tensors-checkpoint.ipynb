{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dkaratzas/DL2022-23/blob/main/Problems%202%20-%20Using%20Autograd%20and%20PyTorch/P2_2_Intro_Tensors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcvesegFyIX-"
   },
   "source": [
    "[![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/dkaratzas/DL2022-23/blob/main/Problems%202%20-%20Using%20Autograd%20and%20PyTorch/P2_2_Intro_Tensors.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IF4i31_7jbg"
   },
   "source": [
    "# What is PyTorch?\n",
    "\n",
    "<a href=\"https://pytorch.org/\">Pytorch</a> is a Python based scientific computing package targeted at two types of audience:\n",
    "\n",
    "-  At the low level, it is a tensor library capable to exploit the computational power of GPUs\n",
    "-  At the high level, it is a deep learning research platform that provides maximum flexibility and speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ng2mpMYgkpu"
   },
   "source": [
    "## Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "FzX92S587jbm"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y62dQH467jbn"
   },
   "source": [
    "## Getting help in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ll9zxJ9IyIYB"
   },
   "source": [
    "The fastest way to get some quick help on something using Jupyter is to just ask! Type any Python object name you want followed by a question mark `?` and the code documentation will be loaded in your notebook. Try it with `torch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "LN6xGdhUyIYB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        module\n",
       "\u001b[1;31mString form:\u001b[0m <module 'torch' from 'D:\\\\musta\\\\envs\\\\pytorch\\\\Lib\\\\site-packages\\\\torch\\\\__init__.py'>\n",
       "\u001b[1;31mFile:\u001b[0m        d:\\musta\\envs\\pytorch\\lib\\site-packages\\torch\\__init__.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "The torch package contains data structures for multi-dimensional\n",
       "tensors and defines mathematical operations over these tensors.\n",
       "Additionally, it provides many utilities for efficient serialization of\n",
       "Tensors and arbitrary types, and other useful utilities.\n",
       "\n",
       "It has a CUDA counterpart, that enables you to run your tensor computations\n",
       "on an NVIDIA GPU with compute capability >= 3.0."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLMcdjjuyIYB"
   },
   "source": [
    "The following command will list all objects of torch that with a name that finishes with \"Tensor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "BIqY0-QT7jbo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.BFloat16Tensor\n",
       "torch.BoolTensor\n",
       "torch.ByteTensor\n",
       "torch.CharTensor\n",
       "torch.DoubleTensor\n",
       "torch.FloatTensor\n",
       "torch.HalfTensor\n",
       "torch.IntTensor\n",
       "torch.LongTensor\n",
       "torch.ShortTensor\n",
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In Colab, you can press <esc> to get out of help\n",
    "torch.*Tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpLRB9CFyIYC"
   },
   "source": [
    "If you use Colab, you also have a handy autocomplete feature at hand. For example, start writing a function name, like `torch.sqrt` if you pause after the first few characters a context menu with possible options will appear. Select the term you meant and press Tab or Enter to autocomplete. Note, this will not work in Jupyter Lab / Notebook out of the box, you would need to install an extension to enable this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "4g-D23eg7jbn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch._VariableFunctionsClass.sqrt>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start typing torch.sqr...  wait and then use <Tab> or <Enter> to autocomplete to torch.sqrt()\n",
    "torch.sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcGTutwryIYD"
   },
   "source": [
    "In Jupyter Lab (but not in CoLab) you can access the documentation by clicking on the Python object and pressing `<Shift>` + `<Tab>`. Try it in the line below (if you are using Jupyter Lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ED3Z0RKO7jbo",
    "outputId": "4a8fc200-c0c6-4af2-8bae-848d4ea728b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Module()"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Module()  # <Shift>+<Tab>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGLfjLTgyIYD"
   },
   "source": [
    "You should see the same result as with the line below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "vrBufY4q7jbo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool\n",
       "\u001b[1;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[1;31mFile:\u001b[0m           d:\\musta\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     Identity, Linear, Bilinear, _ConvNd, Threshold, ReLU, RReLU, Hardtanh, Sigmoid, Hardsigmoid, ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Annotate your functions / classes!\n",
    "torch.nn.Module?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQjnow2DyIYE"
   },
   "source": [
    "Where does this documentation come from? Part of it comes from the code itself, and part of it from the annotations (special comments) that are introduced in the function / class definitions. To have a look at the actual code of a function, just use a double `??`. See for example below, and get used to annotating your functions / classes as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "k6gVzBNi7jbp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mSource:\u001b[0m        \n",
       "\u001b[1;32mclass\u001b[0m \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34mr\"\"\"Base class for all neural network modules.\n",
       "\n",
       "    Your models should also subclass this class.\n",
       "\n",
       "    Modules can also contain other Modules, allowing to nest them in\n",
       "    a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "        import torch.nn as nn\n",
       "        import torch.nn.functional as F\n",
       "\n",
       "        class Model(nn.Module):\n",
       "            def __init__(self):\n",
       "                super().__init__()\n",
       "                self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "                self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "            def forward(self, x):\n",
       "                x = F.relu(self.conv1(x))\n",
       "                return F.relu(self.conv2(x))\n",
       "\n",
       "    Submodules assigned in this way will be registered, and will have their\n",
       "    parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       "    .. note::\n",
       "        As per the example above, an ``__init__()`` call to the parent class\n",
       "        must be made before assignment on the child.\n",
       "\n",
       "    :ivar training: Boolean represents whether this module is in training or\n",
       "                    evaluation mode.\n",
       "    :vartype training: bool\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdump_patches\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_version\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34mr\"\"\"This allows better BC support for :meth:`load_state_dict`. In\n",
       "    :meth:`state_dict`, the version number will be saved as in the attribute\n",
       "    `_metadata` of the returned state dict, and thus pickled. `_metadata` is a\n",
       "    dictionary with keys that follow the naming convention of state dict. See\n",
       "    ``_load_from_state_dict`` on how to use this information in loading.\n",
       "\n",
       "    If new parameters/buffers are added/removed from a module, this number shall\n",
       "    be bumped, and the module's `_load_from_state_dict` method can compare the\n",
       "    version number and do appropriate changes if the state dict is from before\n",
       "    the change.\"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_parameters\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_buffers\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_non_persistent_buffers_set\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_backward_pre_hooks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_backward_hooks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_is_full_backward_hook\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_forward_hooks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Marks whether the corresponding _forward_hooks accept kwargs or not.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# As JIT does not support Set[int], this dict is used as a set, where all\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# hooks represented in this dict accept kwargs.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_forward_hooks_with_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# forward hooks that should always be called even if an exception is raised\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_forward_hooks_always_called\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Marks whether the corresponding _forward_hooks accept kwargs or not.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# As JIT does not support Set[int], this dict is used as a set, where all\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# hooks represented in this dict accept kwargs.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_forward_pre_hooks_with_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_state_dict_hooks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_load_state_dict_pre_hooks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_state_dict_pre_hooks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_load_state_dict_post_hooks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_modules\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcall_super_init\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_compiled_call_impl\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"\"\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_api_usage_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"python.nn_module\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# Backward compatibility: no args used to be allowed when call_super_init=False\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_super_init\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}.__init__() got an unexpected keyword argument '{}'\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_super_init\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m.__init__() takes 1 positional argument but \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m were\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[1;34m\" given\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"\"\"\n",
       "        Calls super().__setattr__('a', a) instead of the typical self.a = a\n",
       "        to avoid Module.__setattr__ overhead. Module's __setattr__ has special\n",
       "        handling for parameters, submodules, and buffers but simply calls into\n",
       "        super().__setattr__ for all other attributes.\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_parameters'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_buffers'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_non_persistent_buffers_set'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_backward_pre_hooks'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_backward_hooks'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_is_full_backward_hook'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_forward_hooks'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_forward_hooks_with_kwargs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_forward_hooks_always_called'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_forward_pre_hooks'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_forward_pre_hooks_with_kwargs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_state_dict_hooks'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_state_dict_pre_hooks'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_load_state_dict_pre_hooks'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_load_state_dict_post_hooks'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_modules'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_super_init\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mforward\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_forward_unimplemented\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mregister_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpersistent\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Add a buffer to the module.\n",
       "\n",
       "        This is typically used to register a buffer that should not to be\n",
       "        considered a model parameter. For example, BatchNorm's ``running_mean``\n",
       "        is not a parameter, but is part of the module's state. Buffers, by\n",
       "        default, are persistent and will be saved alongside parameters. This\n",
       "        behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
       "        only difference between a persistent buffer and a non-persistent buffer\n",
       "        is that the latter will not be a part of this module's\n",
       "        :attr:`state_dict`.\n",
       "\n",
       "        Buffers can be accessed as attributes using given names.\n",
       "\n",
       "        Args:\n",
       "            name (str): name of the buffer. The buffer can be accessed\n",
       "                from this module using the given name\n",
       "            tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
       "                that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
       "                the buffer is **not** included in the module's :attr:`state_dict`.\n",
       "            persistent (bool): whether the buffer is part of this module's\n",
       "                :attr:`state_dict`.\n",
       "\n",
       "        Example::\n",
       "\n",
       "            >>> # xdoctest: +SKIP(\"undefined vars\")\n",
       "            >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
       "\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mpersistent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mScriptModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ScriptModule does not support non-persistent buffers\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;34m'_buffers'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;34m\"cannot assign buffer before Module.__init__() call\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mbuffer name should be a string. Got \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[1;34m'.'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"buffer name can't contain \\\".\\\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"buffer name can't be empty string \\\"\\\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mattribute '\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m' already exists\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mcannot assign '\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m' object to buffer '\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m' \u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[1;34m\"(torch Tensor or None required)\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_global_buffer_registration_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mpersistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_non_persistent_buffers_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_non_persistent_buffers_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mregister_parameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Add a parameter to the module.\n",
       "\n",
       "        The parameter can be accessed as an attribute using given name.\n",
       "\n",
       "        Args:\n",
       "            name (str): name of the parameter. The parameter can be accessed\n",
       "                from this module using the given name\n",
       "            param (Parameter or None): parameter to be added to the module. If\n",
       "                ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
       "                are ignored. If ``None``, the parameter is **not** included in the\n",
       "                module's :attr:`state_dict`.\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;34m'_parameters'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;34m\"cannot assign parameter before Module.__init__() call\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mparameter name should be a string. Got \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[1;34m'.'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"parameter name can't contain \\\".\\\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"parameter name can't be empty string \\\"\\\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mattribute '\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m' already exists\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mcannot assign '\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m' object to parameter '\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m' \u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[1;34m\"(torch.nn.Parameter or None required)\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;33mf\"\u001b[0m\u001b[1;33mCannot assign non-leaf Tensor to parameter '\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m'. Model \u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;33mf\"\u001b[0m\u001b[1;33mparameters must be created explicitly. To express '\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m' \u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;34m\"as a function of another Tensor, compute the value in \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;34m\"the forward() method.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_global_parameter_registration_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mparam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0madd_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Add a child module to the current module.\n",
       "\n",
       "        The module can be accessed as an attribute using the given name.\n",
       "\n",
       "        Args:\n",
       "            name (str): name of the child module. The child module can be\n",
       "                accessed from this module using the given name\n",
       "            module (Module): child module to be added to the module.\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m is not a Module subclass\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mmodule name should be a string. Got \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mattribute '\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m' already exists\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[1;34m'.'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mmodule name can't contain \\\".\\\", got: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"module name can't be empty string \\\"\\\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_global_module_registration_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mregister_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Alias for :func:`add_module`.\"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mget_submodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Module\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"\"\"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n",
       "\n",
       "        For example, let's say you have an ``nn.Module`` ``A`` that\n",
       "        looks like this:\n",
       "\n",
       "        .. code-block:: text\n",
       "\n",
       "            A(\n",
       "                (net_b): Module(\n",
       "                    (net_c): Module(\n",
       "                        (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
       "                    )\n",
       "                    (linear): Linear(in_features=100, out_features=200, bias=True)\n",
       "                )\n",
       "            )\n",
       "\n",
       "        (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
       "        submodule ``net_b``, which itself has two submodules ``net_c``\n",
       "        and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
       "\n",
       "        To check whether or not we have the ``linear`` submodule, we\n",
       "        would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
       "        we have the ``conv`` submodule, we would call\n",
       "        ``get_submodule(\"net_b.net_c.conv\")``.\n",
       "\n",
       "        The runtime of ``get_submodule`` is bounded by the degree\n",
       "        of module nesting in ``target``. A query against\n",
       "        ``named_modules`` achieves the same result, but it is O(N) in\n",
       "        the number of transitive modules. So, for a simple check to see\n",
       "        if some submodule exists, ``get_submodule`` should always be\n",
       "        used.\n",
       "\n",
       "        Args:\n",
       "            target: The fully-qualified string name of the submodule\n",
       "                to look for. (See above example for how to specify a\n",
       "                fully-qualified string.)\n",
       "\n",
       "        Returns:\n",
       "            torch.nn.Module: The submodule referenced by ``target``\n",
       "\n",
       "        Raises:\n",
       "            AttributeError: If the target string references an invalid\n",
       "                path or resolves to something that is not an\n",
       "                ``nn.Module``\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0matoms\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mmod\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0matoms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" has no \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                     \u001b[1;34m\"attribute `\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"`\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"`\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"` is not \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                     \u001b[1;34m\"an nn.Module\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mget_parameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Parameter\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"\"\"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n",
       "\n",
       "        See the docstring for ``get_submodule`` for a more detailed\n",
       "        explanation of this method's functionality as well as how to\n",
       "        correctly specify ``target``.\n",
       "\n",
       "        Args:\n",
       "            target: The fully-qualified string name of the Parameter\n",
       "                to look for. (See ``get_submodule`` for how to specify a\n",
       "                fully-qualified string.)\n",
       "\n",
       "        Returns:\n",
       "            torch.nn.Parameter: The Parameter referenced by ``target``\n",
       "\n",
       "        Raises:\n",
       "            AttributeError: If the target string references an invalid\n",
       "                path or resolves to something that is not an\n",
       "                ``nn.Parameter``\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mmodule_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mmod\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_submodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" has no attribute `\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                 \u001b[1;33m+\u001b[0m \u001b[0mparam_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"`\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mparam\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"`\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mparam_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"` is not an \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                 \u001b[1;34m\"nn.Parameter\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mget_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Tensor\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"\"\"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n",
       "\n",
       "        See the docstring for ``get_submodule`` for a more detailed\n",
       "        explanation of this method's functionality as well as how to\n",
       "        correctly specify ``target``.\n",
       "\n",
       "        Args:\n",
       "            target: The fully-qualified string name of the buffer\n",
       "                to look for. (See ``get_submodule`` for how to specify a\n",
       "                fully-qualified string.)\n",
       "\n",
       "        Returns:\n",
       "            torch.Tensor: The buffer referenced by ``target``\n",
       "\n",
       "        Raises:\n",
       "            AttributeError: If the target string references an invalid\n",
       "                path or resolves to something that is not a\n",
       "                buffer\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mmodule_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mmod\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_submodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" has no attribute `\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                 \u001b[1;33m+\u001b[0m \u001b[0mbuffer_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"`\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mbuffer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mbuffer_name\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"`\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbuffer_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"` is not a buffer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mget_extra_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"\"\"Return any extra state to include in the module's state_dict.\n",
       "\n",
       "        Implement this and a corresponding :func:`set_extra_state` for your module\n",
       "        if you need to store extra state. This function is called when building the\n",
       "        module's `state_dict()`.\n",
       "\n",
       "        Note that extra state should be picklable to ensure working serialization\n",
       "        of the state_dict. We only provide provide backwards compatibility guarantees\n",
       "        for serializing Tensors; other objects may break backwards compatibility if\n",
       "        their serialized pickled form changes.\n",
       "\n",
       "        Returns:\n",
       "            object: Any extra state to store in the module's state_dict\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;34m\"Reached a code path in Module.get_extra_state() that should never be called. \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;34m\"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;34m\"to report this bug.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mset_extra_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"\"\"Set extra state contained in the loaded `state_dict`.\n",
       "\n",
       "        This function is called from :func:`load_state_dict` to handle any extra state\n",
       "        found within the `state_dict`. Implement this function and a corresponding\n",
       "        :func:`get_extra_state` for your module if you need to store extra state within its\n",
       "        `state_dict`.\n",
       "\n",
       "        Args:\n",
       "            state (dict): Extra state from the `state_dict`\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;34m\"Reached a code path in Module.set_extra_state() that should never be called. \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;34m\"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;34m\"to report this bug.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_compatible_shallow_copy_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# If the new tensor has compatible tensor type as the existing tensor,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# the current behavior is to change the tensor in-place using `.data =`,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# and the future behavior is to overwrite the existing tensor. However,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# changing the current behavior is a BC-breaking change, and we want it\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# to happen in future releases. So for now we introduce the\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# `torch.__future__.get_overwrite_module_params_on_conversion()`\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# global flag to let the user control whether they want the future\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# behavior of overwriting the existing tensor or not.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mreturn\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__future__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_overwrite_module_params_on_conversion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# Tensors stored in modules are graph leaves, and we don't want to\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# track autograd history of `param_applied`, so we have to use\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mout_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32massert\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mout_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_applied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_param\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mgrad_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32massert\u001b[0m \u001b[0mout_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mout_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_applied\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32massert\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mout_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_applied\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n",
       "\n",
       "        Typical use includes initializing the parameters of a model\n",
       "        (see also :ref:`nn-init-doc`).\n",
       "\n",
       "        Args:\n",
       "            fn (:class:`Module` -> None): function to be applied to each submodule\n",
       "\n",
       "        Returns:\n",
       "            Module: self\n",
       "\n",
       "        Example::\n",
       "\n",
       "            >>> @torch.no_grad()\n",
       "            >>> def init_weights(m):\n",
       "            >>>     print(m)\n",
       "            >>>     if type(m) == nn.Linear:\n",
       "            >>>         m.weight.fill_(1.0)\n",
       "            >>>         print(m.weight)\n",
       "            >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
       "            >>> net.apply(init_weights)\n",
       "            Linear(in_features=2, out_features=2, bias=True)\n",
       "            Parameter containing:\n",
       "            tensor([[1., 1.],\n",
       "                    [1., 1.]], requires_grad=True)\n",
       "            Linear(in_features=2, out_features=2, bias=True)\n",
       "            Parameter containing:\n",
       "            tensor([[1., 1.],\n",
       "                    [1., 1.]], requires_grad=True)\n",
       "            Sequential(\n",
       "              (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "              (1): Linear(in_features=2, out_features=2, bias=True)\n",
       "            )\n",
       "\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Move all model parameters and buffers to the GPU.\n",
       "\n",
       "        This also makes associated parameters and buffers different objects. So\n",
       "        it should be called before constructing optimizer if the module will\n",
       "        live on GPU while being optimized.\n",
       "\n",
       "        .. note::\n",
       "            This method modifies the module in-place.\n",
       "\n",
       "        Args:\n",
       "            device (int, optional): if specified, all parameters will be\n",
       "                copied to that device\n",
       "\n",
       "        Returns:\n",
       "            Module: self\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Move all model parameters and buffers to the IPU.\n",
       "\n",
       "        This also makes associated parameters and buffers different objects. So\n",
       "        it should be called before constructing optimizer if the module will\n",
       "        live on IPU while being optimized.\n",
       "\n",
       "        .. note::\n",
       "            This method modifies the module in-place.\n",
       "\n",
       "        Arguments:\n",
       "            device (int, optional): if specified, all parameters will be\n",
       "                copied to that device\n",
       "\n",
       "        Returns:\n",
       "            Module: self\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Move all model parameters and buffers to the XPU.\n",
       "\n",
       "        This also makes associated parameters and buffers different objects. So\n",
       "        it should be called before constructing optimizer if the module will\n",
       "        live on XPU while being optimized.\n",
       "\n",
       "        .. note::\n",
       "            This method modifies the module in-place.\n",
       "\n",
       "        Arguments:\n",
       "            device (int, optional): if specified, all parameters will be\n",
       "                copied to that device\n",
       "\n",
       "        Returns:\n",
       "            Module: self\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Move all model parameters and buffers to the CPU.\n",
       "\n",
       "        .. note::\n",
       "            This method modifies the module in-place.\n",
       "\n",
       "        Returns:\n",
       "            Module: self\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Casts all parameters and buffers to :attr:`dst_type`.\n",
       "\n",
       "        .. note::\n",
       "            This method modifies the module in-place.\n",
       "\n",
       "        Args:\n",
       "            dst_type (type or string): the desired type\n",
       "\n",
       "        Returns:\n",
       "            Module: self\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Casts all floating point parameters and buffers to ``float`` datatype.\n",
       "\n",
       "        .. note::\n",
       "            This method modifies the module in-place.\n",
       "\n",
       "        Returns:\n",
       "            Module: self\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Casts all floating point parameters and buffers to ``double`` datatype.\n",
       "\n",
       "        .. note::\n",
       "            This method modifies the module in-place.\n",
       "\n",
       "        Returns:\n",
       "            Module: self\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Casts all floating point parameters and buffers to ``half`` datatype.\n",
       "\n",
       "        .. note::\n",
       "            This method modifies the module in-place.\n",
       "\n",
       "        Returns:\n",
       "            Module: self\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mbfloat16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
       "\n",
       "        .. note::\n",
       "            This method modifies the module in-place.\n",
       "\n",
       "        Returns:\n",
       "            Module: self\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mto_empty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDeviceLikeType\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Move the parameters and buffers to the specified device without copying storage.\n",
       "\n",
       "        Args:\n",
       "            device (:class:`torch.device`): The desired device of the parameters\n",
       "                and buffers in this module.\n",
       "            recurse (bool): Whether parameters and buffers of submodules should\n",
       "                be recursively moved to the specified device.\n",
       "\n",
       "        Returns:\n",
       "            Module: self\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDeviceLikeType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m           \u001b[0mnon_blocking\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m...\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m...\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m...\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Move and/or cast the parameters and buffers.\n",
       "\n",
       "        This can be called as\n",
       "\n",
       "        .. function:: to(device=None, dtype=None, non_blocking=False)\n",
       "           :noindex:\n",
       "\n",
       "        .. function:: to(dtype, non_blocking=False)\n",
       "           :noindex:\n",
       "\n",
       "        .. function:: to(tensor, non_blocking=False)\n",
       "           :noindex:\n",
       "\n",
       "        .. function:: to(memory_format=torch.channels_last)\n",
       "           :noindex:\n",
       "\n",
       "        Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
       "        floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
       "        only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
       "        (if given). The integral parameters and buffers will be moved\n",
       "        :attr:`device`, if that is given, but with dtypes unchanged. When\n",
       "        :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
       "        with respect to the host if possible, e.g., moving CPU Tensors with\n",
       "        pinned memory to CUDA devices.\n",
       "\n",
       "        See below for examples.\n",
       "\n",
       "        .. note::\n",
       "            This method modifies the module in-place.\n",
       "\n",
       "        Args:\n",
       "            device (:class:`torch.device`): the desired device of the parameters\n",
       "                and buffers in this module\n",
       "            dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
       "                the parameters and buffers in this module\n",
       "            tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
       "                dtype and device for all parameters and buffers in this module\n",
       "            memory_format (:class:`torch.memory_format`): the desired memory\n",
       "                format for 4D parameters and buffers in this module (keyword\n",
       "                only argument)\n",
       "\n",
       "        Returns:\n",
       "            Module: self\n",
       "\n",
       "        Examples::\n",
       "\n",
       "            >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
       "            >>> linear = nn.Linear(2, 2)\n",
       "            >>> linear.weight\n",
       "            Parameter containing:\n",
       "            tensor([[ 0.1913, -0.3420],\n",
       "                    [-0.5113, -0.2325]])\n",
       "            >>> linear.to(torch.double)\n",
       "            Linear(in_features=2, out_features=2, bias=True)\n",
       "            >>> linear.weight\n",
       "            Parameter containing:\n",
       "            tensor([[ 0.1913, -0.3420],\n",
       "                    [-0.5113, -0.2325]], dtype=torch.float64)\n",
       "            >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
       "            >>> gpu1 = torch.device(\"cuda:1\")\n",
       "            >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
       "            Linear(in_features=2, out_features=2, bias=True)\n",
       "            >>> linear.weight\n",
       "            Parameter containing:\n",
       "            tensor([[ 0.1914, -0.3420],\n",
       "                    [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
       "            >>> cpu = torch.device(\"cpu\")\n",
       "            >>> linear.to(cpu)\n",
       "            Linear(in_features=2, out_features=2, bias=True)\n",
       "            >>> linear.weight\n",
       "            Parameter containing:\n",
       "            tensor([[ 0.1914, -0.3420],\n",
       "                    [-0.5112, -0.2324]], dtype=torch.float16)\n",
       "\n",
       "            >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
       "            >>> linear.weight\n",
       "            Parameter containing:\n",
       "            tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
       "                    [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
       "            >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
       "            tensor([[0.6122+0.j, 0.1150+0.j],\n",
       "                    [0.6122+0.j, 0.1150+0.j],\n",
       "                    [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
       "\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nn.Module.to only accepts floating point or complex '\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                \u001b[1;33mf'\u001b[0m\u001b[1;33mdtypes, but got desired dtype=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;34m\"Complex modules are a new feature under active development whose design may change, \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;34m\"and some modules might not work as expected when using complex tensors as parameters or buffers. \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;34m\"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;34m\"if a complex module does not work as expected.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[0mnon_blocking\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mregister_full_backward_pre_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mhook\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Module\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_grad_t\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_grad_t\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mprepend\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mRemovableHandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Register a backward pre-hook on the module.\n",
       "\n",
       "        The hook will be called every time the gradients for the module are computed.\n",
       "        The hook should have the following signature::\n",
       "\n",
       "            hook(module, grad_output) -> tuple[Tensor] or None\n",
       "\n",
       "        The :attr:`grad_output` is a tuple. The hook should\n",
       "        not modify its arguments, but it can optionally return a new gradient with\n",
       "        respect to the output that will be used in place of :attr:`grad_output` in\n",
       "        subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
       "        all non-Tensor arguments.\n",
       "\n",
       "        For technical reasons, when this hook is applied to a Module, its forward function will\n",
       "        receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
       "        of each Tensor returned by the Module's forward function.\n",
       "\n",
       "        .. warning ::\n",
       "            Modifying inputs inplace is not allowed when using backward hooks and\n",
       "            will raise an error.\n",
       "\n",
       "        Args:\n",
       "            hook (Callable): The user-defined hook to be registered.\n",
       "            prepend (bool): If true, the provided ``hook`` will be fired before\n",
       "                all existing ``backward_pre`` hooks on this\n",
       "                :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
       "                ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
       "                on this :class:`torch.nn.modules.Module`. Note that global\n",
       "                ``backward_pre`` hooks registered with\n",
       "                :func:`register_module_full_backward_pre_hook` will fire before\n",
       "                all hooks registered by this method.\n",
       "\n",
       "        Returns:\n",
       "            :class:`torch.utils.hooks.RemovableHandle`:\n",
       "                a handle that can be used to remove the added hook by calling\n",
       "                ``handle.remove()``\n",
       "\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRemovableHandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_pre_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_pre_hooks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mprepend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove_to_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Module'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_grad_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_grad_t\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_grad_t\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mRemovableHandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Register a backward hook on the module.\n",
       "\n",
       "        This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
       "        the behavior of this function will change in future versions.\n",
       "\n",
       "        Returns:\n",
       "            :class:`torch.utils.hooks.RemovableHandle`:\n",
       "                a handle that can be used to remove the added hook by calling\n",
       "                ``handle.remove()``\n",
       "\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_full_backward_hook\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot use both regular backward hooks and full backward hooks on a \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                               \u001b[1;34m\"single Module. Please use only one of them.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_full_backward_hook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRemovableHandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_hooks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mregister_full_backward_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mhook\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Module\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_grad_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_grad_t\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_grad_t\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mprepend\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mRemovableHandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Register a backward hook on the module.\n",
       "\n",
       "        The hook will be called every time the gradients with respect to a module\n",
       "        are computed, i.e. the hook will execute if and only if the gradients with\n",
       "        respect to module outputs are computed. The hook should have the following\n",
       "        signature::\n",
       "\n",
       "            hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
       "\n",
       "        The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
       "        with respect to the inputs and outputs respectively. The hook should\n",
       "        not modify its arguments, but it can optionally return a new gradient with\n",
       "        respect to the input that will be used in place of :attr:`grad_input` in\n",
       "        subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
       "        as positional arguments and all kwarg arguments are ignored. Entries\n",
       "        in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
       "        arguments.\n",
       "\n",
       "        For technical reasons, when this hook is applied to a Module, its forward function will\n",
       "        receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
       "        of each Tensor returned by the Module's forward function.\n",
       "\n",
       "        .. warning ::\n",
       "            Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
       "            will raise an error.\n",
       "\n",
       "        Args:\n",
       "            hook (Callable): The user-defined hook to be registered.\n",
       "            prepend (bool): If true, the provided ``hook`` will be fired before\n",
       "                all existing ``backward`` hooks on this\n",
       "                :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
       "                ``hook`` will be fired after all existing ``backward`` hooks on\n",
       "                this :class:`torch.nn.modules.Module`. Note that global\n",
       "                ``backward`` hooks registered with\n",
       "                :func:`register_module_full_backward_hook` will fire before\n",
       "                all hooks registered by this method.\n",
       "\n",
       "        Returns:\n",
       "            :class:`torch.utils.hooks.RemovableHandle`:\n",
       "                a handle that can be used to remove the added hook by calling\n",
       "                ``handle.remove()``\n",
       "\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_full_backward_hook\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot use both regular backward hooks and full backward hooks on a \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                               \u001b[1;34m\"single Module. Please use only one of them.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_full_backward_hook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRemovableHandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_hooks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mprepend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove_to_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_get_backward_hooks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Return the backward hooks for use in the call function.\n",
       "\n",
       "        It returns two lists, one with the full backward hooks and one with the non-full\n",
       "        backward hooks.\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_global_is_full_backward_hook\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mfull_backward_hooks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_full_backward_hook\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mfull_backward_hooks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mnon_full_backward_hooks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_global_is_full_backward_hook\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_full_backward_hook\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_get_backward_pre_hooks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mbackward_pre_hooks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mbackward_pre_hooks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mbackward_pre_hooks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mbackward_pre_hooks\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_maybe_warn_non_full_backward_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Using non-full backward hooks on a Module that does not return a \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                              \u001b[1;34m\"single Tensor or a tuple of Tensors is deprecated and will be removed \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                              \u001b[1;34m\"in future versions. This hook will be missing some of the grad_output. \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                              \u001b[1;34m\"Please use register_full_backward_hook to get the documented behavior.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mreturn\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Using non-full backward hooks on a Module that does not take as input a \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                              \u001b[1;34m\"single Tensor or a tuple of Tensors is deprecated and will be removed \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                              \u001b[1;34m\"in future versions. This hook will be missing some of the grad_input. \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                              \u001b[1;34m\"Please use register_full_backward_hook to get the documented behavior.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mreturn\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# At this point we are sure that inputs and result are tuple of Tensors\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mout_grad_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_fn\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_grad_fn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_grad_fn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgrad_fn\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mout_grad_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Using a non-full backward hook when outputs are nested in python data structure \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                          \u001b[1;34m\"is deprecated and will be removed in future versions. This hook will be missing \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                          \u001b[1;34m\"some grad_output.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_grad_fn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Using a non-full backward hook when outputs are generated by different autograd Nodes \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                          \u001b[1;34m\"is deprecated and will be removed in future versions. This hook will be missing \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                          \u001b[1;34m\"some grad_output. Please use register_full_backward_hook to get the documented behavior.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# At this point the grad_output part of the hook will most likely be correct\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0minputs_grad_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_fn\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mnext_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0minputs_grad_fn\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnext_functions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                              \u001b[1;34m\"is deprecated and will be removed in future versions. This hook will be missing \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                              \u001b[1;34m\"some grad_input. Please use register_full_backward_hook to get the documented \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                              \u001b[1;34m\"behavior.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mregister_forward_pre_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mhook\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mprepend\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mwith_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mRemovableHandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Register a forward pre-hook on the module.\n",
       "\n",
       "        The hook will be called every time before :func:`forward` is invoked.\n",
       "\n",
       "\n",
       "        If ``with_kwargs`` is false or not specified, the input contains only\n",
       "        the positional arguments given to the module. Keyword arguments won't be\n",
       "        passed to the hooks and only to the ``forward``. The hook can modify the\n",
       "        input. User can either return a tuple or a single modified value in the\n",
       "        hook. We will wrap the value into a tuple if a single value is returned\n",
       "        (unless that value is already a tuple). The hook should have the\n",
       "        following signature::\n",
       "\n",
       "            hook(module, args) -> None or modified input\n",
       "\n",
       "        If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
       "        kwargs given to the forward function. And if the hook modifies the\n",
       "        input, both the args and kwargs should be returned. The hook should have\n",
       "        the following signature::\n",
       "\n",
       "            hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
       "\n",
       "        Args:\n",
       "            hook (Callable): The user defined hook to be registered.\n",
       "            prepend (bool): If true, the provided ``hook`` will be fired before\n",
       "                all existing ``forward_pre`` hooks on this\n",
       "                :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
       "                ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
       "                on this :class:`torch.nn.modules.Module`. Note that global\n",
       "                ``forward_pre`` hooks registered with\n",
       "                :func:`register_module_forward_pre_hook` will fire before all\n",
       "                hooks registered by this method.\n",
       "                Default: ``False``\n",
       "            with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
       "                given to the forward function.\n",
       "                Default: ``False``\n",
       "\n",
       "        Returns:\n",
       "            :class:`torch.utils.hooks.RemovableHandle`:\n",
       "                a handle that can be used to remove the added hook by calling\n",
       "                ``handle.remove()``\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRemovableHandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mextra_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks_with_kwargs\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mwith_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks_with_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mprepend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove_to_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mregister_forward_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mhook\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mprepend\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mwith_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0malways_call\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mRemovableHandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Register a forward hook on the module.\n",
       "\n",
       "        The hook will be called every time after :func:`forward` has computed an output.\n",
       "\n",
       "        If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
       "        the positional arguments given to the module. Keyword arguments won't be\n",
       "        passed to the hooks and only to the ``forward``. The hook can modify the\n",
       "        output. It can modify the input inplace but it will not have effect on\n",
       "        forward since this is called after :func:`forward` is called. The hook\n",
       "        should have the following signature::\n",
       "\n",
       "            hook(module, args, output) -> None or modified output\n",
       "\n",
       "        If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
       "        ``kwargs`` given to the forward function and be expected to return the\n",
       "        output possibly modified. The hook should have the following signature::\n",
       "\n",
       "            hook(module, args, kwargs, output) -> None or modified output\n",
       "\n",
       "        Args:\n",
       "            hook (Callable): The user defined hook to be registered.\n",
       "            prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
       "                before all existing ``forward`` hooks on this\n",
       "                :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
       "                ``hook`` will be fired after all existing ``forward`` hooks on\n",
       "                this :class:`torch.nn.modules.Module`. Note that global\n",
       "                ``forward`` hooks registered with\n",
       "                :func:`register_module_forward_hook` will fire before all hooks\n",
       "                registered by this method.\n",
       "                Default: ``False``\n",
       "            with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
       "                kwargs given to the forward function.\n",
       "                Default: ``False``\n",
       "            always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
       "                whether an exception is raised while calling the Module.\n",
       "                Default: ``False``\n",
       "\n",
       "        Returns:\n",
       "            :class:`torch.utils.hooks.RemovableHandle`:\n",
       "                a handle that can be used to remove the added hook by calling\n",
       "                ``handle.remove()``\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRemovableHandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mextra_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks_with_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks_always_called\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mwith_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks_with_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0malways_call\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks_always_called\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mprepend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove_to_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mtracing_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtracing_state\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mScriptMethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mrecording_scopes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trace_module_map\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# type ignore was added because at this point one knows that\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trace_module_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trace_module_map\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# type: ignore[index, operator] # noqa: B950\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mtracing_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mrecording_scopes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mtracing_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_wrapped_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mforward_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# this function, and just call forward.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mor\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mcalled_always_called_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mbackward_pre_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mbackward_pre_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_backward_pre_hooks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_backward_hooks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mfor\u001b[0m \u001b[0mhook_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;33m*\u001b[0m\u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mif\u001b[0m \u001b[0mhook_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks_with_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0margs_kwargs_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;32mif\u001b[0m \u001b[0margs_kwargs_result\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs_kwargs_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs_kwargs_result\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs_kwargs_result\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                    \u001b[1;34m\"forward pre-hook must return None or a tuple \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                    \u001b[1;33mf\"\u001b[0m\u001b[1;33mof (new_args, new_kwargs), but got \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0margs_kwargs_result\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0margs_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;32mif\u001b[0m \u001b[0margs_result\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                \u001b[0margs_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs_result\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs_result\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mbw_hook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mfull_backward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mbackward_pre_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mbw_hook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBackwardHook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackward_pre_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mfor\u001b[0m \u001b[0mhook_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;33m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;31m# mark that always called hook is run\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mif\u001b[0m \u001b[0mhook_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks_always_called\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mhook_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_global_forward_hooks_always_called\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mcalled_always_called_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mif\u001b[0m \u001b[0mhook_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks_with_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook_result\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"For backward hooks to be called,\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                  \u001b[1;34m\" module output should be a Tensor or a tuple of Tensors\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                  \u001b[1;33mf\"\u001b[0m\u001b[1;33m but received \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_output_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# Handle the non-full backward hooks\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mgrad_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mgrad_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mgrad_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_WrappedHook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_warn_non_full_backward_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# run always called hooks if they have not already been run\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# For now only forward hooks have the always_call option but perhaps\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# this functionality should be added to full backward hooks as well.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mhook_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mhook_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_global_forward_hooks_always_called\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhook_id\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcalled_always_called_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook_result\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"global module forward hook with ``always_call=True`` raised an exception \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                      \u001b[1;33mf\"\u001b[0m\u001b[1;33mthat was silenced as another error was raised in forward: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mhook_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mhook_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks_always_called\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhook_id\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcalled_always_called_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;32mif\u001b[0m \u001b[0mhook_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks_with_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook_result\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"module forward hook with ``always_call=True`` raised an exception \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                      \u001b[1;33mf\"\u001b[0m\u001b[1;33mthat was silenced as another error was raised in forward: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# raise exception raised in try block\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m__call__\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapped_call_impl\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_compiled_call_impl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# Support loading old checkpoints that don't have the following attrs:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;34m'_forward_pre_hooks'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;34m'_forward_pre_hooks_with_kwargs'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks_with_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;34m'_forward_hooks_with_kwargs'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks_with_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;34m'_forward_hooks_always_called'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks_always_called\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;34m'_state_dict_hooks'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state_dict_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;34m'_state_dict_pre_hooks'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state_dict_pre_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;34m'_load_state_dict_pre_hooks'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_state_dict_pre_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;34m'_load_state_dict_post_hooks'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_state_dict_post_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;34m'_non_persistent_buffers_set'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_non_persistent_buffers_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;34m'_is_full_backward_hook'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_full_backward_hook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;34m'_backward_pre_hooks'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_pre_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# On the return type:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# This is done for better interop with various type checkers for the end users.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Having a stricter return type doesn't play nicely with `register_buffer()` and forces\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# people to excessively use type-ignores, asserts, casts, etc.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# See full discussion on the problems with returning `Union` here\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# https://github.com/microsoft/pyright/issues/4213\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;34m'_parameters'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0m_parameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_parameters'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mreturn\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;34m'_buffers'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0m_buffers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_buffers'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_buffers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mreturn\u001b[0m \u001b[0m_buffers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;34m'_modules'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mmodules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_modules'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33m'\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m' object has no attribute '\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m'\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mdef\u001b[0m \u001b[0mremove_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdicts_or_sets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdicts_or_sets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;32mdel\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_parameters'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;34m\"cannot assign parameters before Module.__init__() call\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mremove_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_non_persistent_buffers_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mcannot assign '\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m' as parameter '\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m' \u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                \u001b[1;34m\"(torch.nn.Parameter or None expected)\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mmodules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_modules'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mmodules\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;34m\"cannot assign module before Module.__init__() call\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mremove_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_non_persistent_buffers_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_global_module_registration_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32melif\u001b[0m \u001b[0mmodules\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mcannot assign '\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m' as child module '\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m' \u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                    \u001b[1;34m\"(torch.nn.Module or None expected)\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                    \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_global_module_registration_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mbuffers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_buffers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mcannot assign '\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m' as buffer '\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m' \u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                        \u001b[1;34m\"(torch.Tensor or None expected)\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                        \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_global_buffer_registration_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mbuffers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__delattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_non_persistent_buffers_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__delattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_register_state_dict_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Register a state-dict hook.\n",
       "\n",
       "        These hooks will be called with arguments: `self`, `state_dict`,\n",
       "        `prefix`, `local_metadata`, after the `state_dict` of `self` is set.\n",
       "        Note that only parameters and buffers of `self` or its children are\n",
       "        guaranteed to exist in `state_dict`. The hooks may modify `state_dict`\n",
       "        inplace or return a new one.\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRemovableHandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state_dict_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state_dict_hooks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mregister_state_dict_pre_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Register a pre-hook for the :meth:`~torch.nn.Module.load_state_dict` method.\n",
       "\n",
       "        These hooks will be called with arguments: ``self``, ``prefix``,\n",
       "        and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
       "        hooks can be used to perform pre-processing before the ``state_dict``\n",
       "        call is made.\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRemovableHandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state_dict_pre_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state_dict_pre_hooks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_save_to_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Save module state to the `destination` dictionary.\n",
       "\n",
       "        The `destination` dictionary will contain the state\n",
       "        of the module, but not its descendants. This is called on every\n",
       "        submodule in :meth:`~torch.nn.Module.state_dict`.\n",
       "\n",
       "        In rare cases, subclasses can achieve class-specific behavior by\n",
       "        overriding this method with custom logic.\n",
       "\n",
       "        Args:\n",
       "            destination (dict): a dict where state will be stored\n",
       "            prefix (str): the prefix for parameters and buffers used in this\n",
       "                module\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mdestination\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkeep_vars\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_non_persistent_buffers_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mdestination\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuf\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkeep_vars\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mextra_state_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0m_EXTRA_STATE_KEY_SUFFIX\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"get_extra_state\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_extra_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mModule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_extra_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mdestination\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mextra_state_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_extra_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# The user can pass an optional arbitrary mappable object to `state_dict`, in which case `state_dict` returns\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# back that same object. But if they pass nothing, an `OrderedDict` is created and returned.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mT_destination\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'T_destination'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT_destination\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_vars\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT_destination\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m...\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_vars\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m...\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# TODO: Change `*args` to `*` and remove the corresponding warning in docs when BC allows.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Also remove the logic for arg parsing together.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Return a dictionary containing references to the whole state of the module.\n",
       "\n",
       "        Both parameters and persistent buffers (e.g. running averages) are\n",
       "        included. Keys are corresponding parameter and buffer names.\n",
       "        Parameters and buffers set to ``None`` are not included.\n",
       "\n",
       "        .. note::\n",
       "            The returned object is a shallow copy. It contains references\n",
       "            to the module's parameters and buffers.\n",
       "\n",
       "        .. warning::\n",
       "            Currently ``state_dict()`` also accepts positional arguments for\n",
       "            ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
       "            this is being deprecated and keyword arguments will be enforced in\n",
       "            future releases.\n",
       "\n",
       "        .. warning::\n",
       "            Please avoid the use of argument ``destination`` as it is not\n",
       "            designed for end-users.\n",
       "\n",
       "        Args:\n",
       "            destination (dict, optional): If provided, the state of module will\n",
       "                be updated into the dict and the same object is returned.\n",
       "                Otherwise, an ``OrderedDict`` will be created and returned.\n",
       "                Default: ``None``.\n",
       "            prefix (str, optional): a prefix added to parameter and buffer\n",
       "                names to compose the keys in state_dict. Default: ``''``.\n",
       "            keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
       "                returned in the state dict are detached from autograd. If it's\n",
       "                set to ``True``, detaching will not be performed.\n",
       "                Default: ``False``.\n",
       "\n",
       "        Returns:\n",
       "            dict:\n",
       "                a dictionary containing a whole state of the module\n",
       "\n",
       "        Example::\n",
       "\n",
       "            >>> # xdoctest: +SKIP(\"undefined vars\")\n",
       "            >>> module.state_dict().keys()\n",
       "            ['bias', 'weight']\n",
       "\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# TODO: Remove `args` and the parsing logic when BC allows.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mdestination\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mdestination\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mkeep_vars\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mkeep_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# DeprecationWarning is ignored by default\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;34m\"Positional args are being deprecated, use kwargs instead. Refer to \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;34m\"https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;34m\" for details.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mdestination\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mdestination\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mdestination\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mlocal_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_version\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mdestination\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocal_metadata\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state_dict_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_to_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdestination\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state_dict_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mdestination\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook_result\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mdestination\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_register_load_state_dict_pre_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_module\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Register a pre-hook for the :meth:`~torch.nn.Module.load_state_dict` method.\n",
       "\n",
       "        These hooks will be called with arguments: `state_dict`, `prefix`,\n",
       "        `local_metadata`, `strict`, `missing_keys`, `unexpected_keys`,\n",
       "        `error_msgs`, before loading `state_dict` into `self`. These arguments\n",
       "        are exactly the same as those of `_load_from_state_dict`.\n",
       "\n",
       "        If ``with_module`` is ``True``, then the first argument to the hook is\n",
       "        an instance of the module.\n",
       "\n",
       "        Arguments:\n",
       "            hook (Callable): Callable hook that will be invoked before\n",
       "                loading the state dict.\n",
       "            with_module (bool, optional): Whether or not to pass the module\n",
       "                instance to the hook as the first parameter.\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRemovableHandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_state_dict_pre_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_state_dict_pre_hooks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_WrappedHook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mwith_module\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mregister_load_state_dict_post_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Register a post hook to be run after module's ``load_state_dict`` is called.\n",
       "\n",
       "        It should have the following signature::\n",
       "            hook(module, incompatible_keys) -> None\n",
       "\n",
       "        The ``module`` argument is the current module that this hook is registered\n",
       "        on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
       "        of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
       "        is a ``list`` of ``str`` containing the missing keys and\n",
       "        ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
       "\n",
       "        The given incompatible_keys can be modified inplace if needed.\n",
       "\n",
       "        Note that the checks performed when calling :func:`load_state_dict` with\n",
       "        ``strict=True`` are affected by modifications the hook makes to\n",
       "        ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
       "        set of keys will result in an error being thrown when ``strict=True``, and\n",
       "        clearing out both missing and unexpected keys will avoid an error.\n",
       "\n",
       "        Returns:\n",
       "            :class:`torch.utils.hooks.RemovableHandle`:\n",
       "                a handle that can be used to remove the added hook by calling\n",
       "                ``handle.remove()``\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRemovableHandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_state_dict_post_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_state_dict_post_hooks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_load_from_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_metadata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                              \u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Copy parameters and buffers from :attr:`state_dict` into only this module, but not its descendants.\n",
       "\n",
       "        This is called on every submodule\n",
       "        in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\n",
       "        module in input :attr:`state_dict` is provided as :attr:`local_metadata`.\n",
       "        For state dicts without metadata, :attr:`local_metadata` is empty.\n",
       "        Subclasses can achieve class-specific backward compatible loading using\n",
       "        the version number at `local_metadata.get(\"version\", None)`.\n",
       "        Additionally, :attr:`local_metadata` can also contain the key\n",
       "        `assign_to_params_buffers` that indicates whether keys should be\n",
       "        assigned their corresponding tensor in the state_dict.\n",
       "\n",
       "        .. note::\n",
       "            :attr:`state_dict` is not the same object as the input\n",
       "            :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\n",
       "            it can be modified.\n",
       "\n",
       "        Args:\n",
       "            state_dict (dict): a dict containing parameters and\n",
       "                persistent buffers.\n",
       "            prefix (str): the prefix for parameters and buffers used in this\n",
       "                module\n",
       "            local_metadata (dict): a dict containing the metadata for this module.\n",
       "                See\n",
       "            strict (bool): whether to strictly enforce that the keys in\n",
       "                :attr:`state_dict` with :attr:`prefix` match the names of\n",
       "                parameters and buffers in this module\n",
       "            missing_keys (list of str): if ``strict=True``, add missing keys to\n",
       "                this list\n",
       "            unexpected_keys (list of str): if ``strict=True``, add unexpected\n",
       "                keys to this list\n",
       "            error_msgs (list of str): error messages should be added to this\n",
       "                list, and will be reported together in\n",
       "                :meth:`~torch.nn.Module.load_state_dict`\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_state_dict_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_metadata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mpersistent_buffers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_non_persistent_buffers_set\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mlocal_name_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpersistent_buffers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mlocal_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlocal_name_params\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0massign_to_params_buffers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocal_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"assign_to_params_buffers\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlocal_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0minput_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_tensor_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0merror_msgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf'\u001b[0m\u001b[1;33mWhile copying the parameter named \"\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\", \u001b[0m\u001b[1;33m'\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                      \u001b[1;34m'expected torch.Tensor or Tensor-like object from checkpoint but '\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                      \u001b[1;33mf'\u001b[0m\u001b[1;33mreceived \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m'\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                      \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# This is used to avoid copying uninitialized parameters into\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# non-lazy modules, since they dont have the hook to do the checks\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# in such case, it will error when accessing the .shape attribute.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mis_param_lazy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_lazy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_param_lazy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0minput_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_param\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_param_lazy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;31m# local shape should match the one in checkpoint\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0merror_msgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'size mismatch for {}: copying a param with shape {} from checkpoint, '\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                      \u001b[1;34m'the shape in current model is {}.'\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                      \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_meta\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minput_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_meta\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0massign_to_params_buffers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf'\u001b[0m\u001b[1;33mfor \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m: copying from a non-meta parameter in the checkpoint to a meta \u001b[0m\u001b[1;33m'\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                  \u001b[1;34m'parameter in the current model, which is a no-op. (Did you mean to '\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                  \u001b[1;34m'pass `assign=True` to assign items in the state dictionary to their '\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                  \u001b[1;34m'corresponding key in the module instead of copying them in place?)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;32mif\u001b[0m \u001b[0massign_to_params_buffers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[1;31m# Shape checks are already done above\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                    \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                            \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0merror_msgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf'\u001b[0m\u001b[1;33mWhile copying the parameter named \"\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\", \u001b[0m\u001b[1;33m'\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                      \u001b[1;33mf'\u001b[0m\u001b[1;33mwhose dimensions in the model are \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m and \u001b[0m\u001b[1;33m'\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                      \u001b[1;33mf'\u001b[0m\u001b[1;33mwhose dimensions in the checkpoint are \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minput_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m, \u001b[0m\u001b[1;33m'\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                      \u001b[1;33mf'\u001b[0m\u001b[1;33man exception occurred : \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m'\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                      \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32melif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mmissing_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mextra_state_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0m_EXTRA_STATE_KEY_SUFFIX\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"set_extra_state\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_extra_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mModule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_extra_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mextra_state_key\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_extra_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mextra_state_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32melif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mmissing_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextra_state_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0mstrict\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mextra_state_key\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0munexpected_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextra_state_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mextra_state_key\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0minput_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0minput_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# get the name of param/buffer/child\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mif\u001b[0m \u001b[0minput_name\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput_name\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlocal_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0munexpected_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mstrict\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0massign\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n",
       "\n",
       "        If :attr:`strict` is ``True``, then\n",
       "        the keys of :attr:`state_dict` must exactly match the keys returned\n",
       "        by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
       "\n",
       "        .. warning::\n",
       "            If :attr:`assign` is ``True`` the optimizer must be created after\n",
       "            the call to :attr:`load_state_dict`.\n",
       "\n",
       "        Args:\n",
       "            state_dict (dict): a dict containing parameters and\n",
       "                persistent buffers.\n",
       "            strict (bool, optional): whether to strictly enforce that the keys\n",
       "                in :attr:`state_dict` match the keys returned by this module's\n",
       "                :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
       "            assign (bool, optional): whether to assign items in the state\n",
       "                dictionary to their corresponding keys in the module instead\n",
       "                of copying them inplace into the module's current parameters and buffers.\n",
       "                When ``False``, the properties of the tensors in the current\n",
       "                module are preserved while when ``True``, the properties of the\n",
       "                Tensors in the state dict are preserved.\n",
       "                Default: ``False``\n",
       "\n",
       "        Returns:\n",
       "            ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
       "                * **missing_keys** is a list of str containing the missing keys\n",
       "                * **unexpected_keys** is a list of str containing the unexpected keys\n",
       "\n",
       "        Note:\n",
       "            If a parameter or buffer is registered as ``None`` and its corresponding key\n",
       "            exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
       "            ``RuntimeError``.\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mExpected state_dict to be dict-like, got \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mmissing_keys\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0munexpected_keys\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0merror_msgs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# copy state_dict so _load_from_state_dict can modify it\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_metadata'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# mypy isn't aware that \"_metadata\" exists in state_dict\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mstate_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetadata\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_state_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mlocal_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0massign\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mlocal_metadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'assign_to_params_buffers'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0massign\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_from_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mlocal_state_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_metadata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mchild_prefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mchild_state_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlocal_state_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild_state_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mincompatible_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_state_dict_post_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincompatible_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32massert\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;34m\"Hooks registered with ``register_load_state_dict_post_hook`` are not\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;34m\"expected to return new values, if incompatible_keys need to be modified,\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;34m\"it should be done inplace.\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mdel\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0merror_msgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Unexpected key(s) in state_dict: {}. '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf'\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m'\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0merror_msgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Missing key(s) in state_dict: {}. '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf'\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m'\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmissing_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Error(s) in loading state_dict for {}:\\n\\t{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\n\\t\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_named_members\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Help yield various names + members of modules.\"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mmemo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mmodules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mremove_duplicate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrecurse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mmodule_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mmembers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mmemo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule_prefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmodule_prefix\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32myield\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Return an iterator over module parameters.\n",
       "\n",
       "        This is typically passed to an optimizer.\n",
       "\n",
       "        Args:\n",
       "            recurse (bool): if True, then yields parameters of this module\n",
       "                and all submodules. Otherwise, yields only parameters that\n",
       "                are direct members of this module.\n",
       "\n",
       "        Yields:\n",
       "            Parameter: module parameter\n",
       "\n",
       "        Example::\n",
       "\n",
       "            >>> # xdoctest: +SKIP(\"undefined vars\")\n",
       "            >>> for param in model.parameters():\n",
       "            >>>     print(type(param), param.size())\n",
       "            <class 'torch.Tensor'> (20L,)\n",
       "            <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
       "\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32myield\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mnamed_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mprefix\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mrecurse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mremove_duplicate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n",
       "\n",
       "        Args:\n",
       "            prefix (str): prefix to prepend to all parameter names.\n",
       "            recurse (bool): if True, then yields parameters of this module\n",
       "                and all submodules. Otherwise, yields only parameters that\n",
       "                are direct members of this module.\n",
       "            remove_duplicate (bool, optional): whether to remove the duplicated\n",
       "                parameters in the result. Defaults to True.\n",
       "\n",
       "        Yields:\n",
       "            (str, Parameter): Tuple containing the name and parameter\n",
       "\n",
       "        Example::\n",
       "\n",
       "            >>> # xdoctest: +SKIP(\"undefined vars\")\n",
       "            >>> for name, param in self.named_parameters():\n",
       "            >>>     if name in ['bias']:\n",
       "            >>>         print(param.size())\n",
       "\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_named_members\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mlambda\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mremove_duplicate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Return an iterator over module buffers.\n",
       "\n",
       "        Args:\n",
       "            recurse (bool): if True, then yields buffers of this module\n",
       "                and all submodules. Otherwise, yields only buffers that\n",
       "                are direct members of this module.\n",
       "\n",
       "        Yields:\n",
       "            torch.Tensor: module buffer\n",
       "\n",
       "        Example::\n",
       "\n",
       "            >>> # xdoctest: +SKIP(\"undefined vars\")\n",
       "            >>> for buf in model.buffers():\n",
       "            >>>     print(type(buf), buf.size())\n",
       "            <class 'torch.Tensor'> (20L,)\n",
       "            <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
       "\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_buffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32myield\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mnamed_buffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\n",
       "\n",
       "        Args:\n",
       "            prefix (str): prefix to prepend to all buffer names.\n",
       "            recurse (bool, optional): if True, then yields buffers of this module\n",
       "                and all submodules. Otherwise, yields only buffers that\n",
       "                are direct members of this module. Defaults to True.\n",
       "            remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
       "\n",
       "        Yields:\n",
       "            (str, torch.Tensor): Tuple containing the name and buffer\n",
       "\n",
       "        Example::\n",
       "\n",
       "            >>> # xdoctest: +SKIP(\"undefined vars\")\n",
       "            >>> for name, buf in self.named_buffers():\n",
       "            >>>     if name in ['running_var']:\n",
       "            >>>         print(buf.size())\n",
       "\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_named_members\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mlambda\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mremove_duplicate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Return an iterator over immediate children modules.\n",
       "\n",
       "        Yields:\n",
       "            Module: a child module\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_children\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32myield\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mnamed_children\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n",
       "\n",
       "        Yields:\n",
       "            (str, Module): Tuple containing a name and child module\n",
       "\n",
       "        Example::\n",
       "\n",
       "            >>> # xdoctest: +SKIP(\"undefined vars\")\n",
       "            >>> for name, module in model.named_children():\n",
       "            >>>     if name in ['conv4', 'conv5']:\n",
       "            >>>         print(module)\n",
       "\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mmemo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mmemo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32myield\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Return an iterator over all modules in the network.\n",
       "\n",
       "        Yields:\n",
       "            Module: a module in the network\n",
       "\n",
       "        Note:\n",
       "            Duplicate modules are returned only once. In the following\n",
       "            example, ``l`` will be returned only once.\n",
       "\n",
       "        Example::\n",
       "\n",
       "            >>> l = nn.Linear(2, 2)\n",
       "            >>> net = nn.Sequential(l, l)\n",
       "            >>> for idx, m in enumerate(net.modules()):\n",
       "            ...     print(idx, '->', m)\n",
       "\n",
       "            0 -> Sequential(\n",
       "              (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "              (1): Linear(in_features=2, out_features=2, bias=True)\n",
       "            )\n",
       "            1 -> Linear(in_features=2, out_features=2, bias=True)\n",
       "\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32myield\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mnamed_modules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\n",
       "\n",
       "        Args:\n",
       "            memo: a memo to store the set of modules already added to the result\n",
       "            prefix: a prefix that will be added to the name of the module\n",
       "            remove_duplicate: whether to remove the duplicated module instances in the result\n",
       "                or not\n",
       "\n",
       "        Yields:\n",
       "            (str, Module): Tuple of name and module\n",
       "\n",
       "        Note:\n",
       "            Duplicate modules are returned only once. In the following\n",
       "            example, ``l`` will be returned only once.\n",
       "\n",
       "        Example::\n",
       "\n",
       "            >>> l = nn.Linear(2, 2)\n",
       "            >>> net = nn.Sequential(l, l)\n",
       "            >>> for idx, m in enumerate(net.named_modules()):\n",
       "            ...     print(idx, '->', m)\n",
       "\n",
       "            0 -> ('', Sequential(\n",
       "              (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "              (1): Linear(in_features=2, out_features=2, bias=True)\n",
       "            ))\n",
       "            1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
       "\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mmemo\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mmemo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mmemo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32myield\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0msubmodule_prefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Set the module in training mode.\n",
       "\n",
       "        This has any effect only on certain modules. See documentations of\n",
       "        particular modules for details of their behaviors in training/evaluation\n",
       "        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
       "        etc.\n",
       "\n",
       "        Args:\n",
       "            mode (bool): whether to set training mode (``True``) or evaluation\n",
       "                         mode (``False``). Default: ``True``.\n",
       "\n",
       "        Returns:\n",
       "            Module: self\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"training mode is expected to be boolean\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Set the module in evaluation mode.\n",
       "\n",
       "        This has any effect only on certain modules. See documentations of\n",
       "        particular modules for details of their behaviors in training/evaluation\n",
       "        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
       "        etc.\n",
       "\n",
       "        This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
       "\n",
       "        See :ref:`locally-disable-grad-doc` for a comparison between\n",
       "        `.eval()` and several similar mechanisms that may be confused with it.\n",
       "\n",
       "        Returns:\n",
       "            Module: self\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Change if autograd should record operations on parameters in this module.\n",
       "\n",
       "        This method sets the parameters' :attr:`requires_grad` attributes\n",
       "        in-place.\n",
       "\n",
       "        This method is helpful for freezing part of the module for finetuning\n",
       "        or training parts of a model individually (e.g., GAN training).\n",
       "\n",
       "        See :ref:`locally-disable-grad-doc` for a comparison between\n",
       "        `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
       "\n",
       "        Args:\n",
       "            requires_grad (bool): whether autograd should record operations on\n",
       "                                  parameters in this module. Default: ``True``.\n",
       "\n",
       "        Returns:\n",
       "            Module: self\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset_to_none\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Reset gradients of all model parameters.\n",
       "\n",
       "        See similar function under :class:`torch.optim.Optimizer` for more context.\n",
       "\n",
       "        Args:\n",
       "            set_to_none (bool): instead of setting to zero, set the grads to None.\n",
       "                See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_is_replica'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;34m\"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;34m\"The parameters are copied (in a differentiable manner) from the original module. \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;34m\"This means they are not leaf nodes in autograd and so don't accumulate gradients. \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;34m\"If you need gradients in your forward method, consider using autograd.grad instead.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mset_to_none\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mshare_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"See :meth:`torch.Tensor.share_memory_`.\"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshare_memory_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_get_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34mr\"\"\"Set the extra representation of the module.\n",
       "\n",
       "        To print customized extra information, you should re-implement\n",
       "        this method in your own modules. Both single-line and multi-line\n",
       "        strings are acceptable.\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# We treat the extra repr like the sub-module, one item per line\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mextra_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mextra_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# empty string will be split into list ['']\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mextra_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mchild_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mmod_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mmod_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_addindent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mchild_lines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'('\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'): '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmod_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextra_lines\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mchild_lines\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mmain_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'('\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# simple one-liner info, which most builtin Modules will use\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextra_lines\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchild_lines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mmain_str\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mextra_lines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mmain_str\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'\\n  '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n  '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mmain_str\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m')'\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mmain_str\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mmodule_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mattrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mmodules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mbuffers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule_attrs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mattrs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodules\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# Eliminate attrs that are not legal Python variable names\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_replicate_for_data_parallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mreplica\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mreplica\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# replicas do not have parameters themselves, the replicas reference the original\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# module.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mreplica\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mreplica\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplica\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mreplica\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplica\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mreplica\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_replica\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m  \u001b[1;31m# type: ignore[assignment]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mreplica\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"\"\"\n",
       "        Compile this Module's forward using :func:`torch.compile`.\n",
       "\n",
       "        This Module's `__call__` method is compiled and all arguments are passed as-is\n",
       "        to :func:`torch.compile`.\n",
       "\n",
       "        See :func:`torch.compile` for details on the arguments for this function.\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m           d:\\musta\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     Identity, Linear, Bilinear, _ConvNd, Threshold, ReLU, RReLU, Hardtanh, Sigmoid, Hardsigmoid, ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.nn.Module??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3odUxKDa7jbq"
   },
   "source": [
    "## Torch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw3ACW-eyIYE"
   },
   "source": [
    "At the core of PyTorch there is the `Tensor` class. It is very much like numpy's arrays, but supports autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxMMJ1tO7jbq",
    "outputId": "61675b34-d95d-4722-e426-dae8d3a1ea2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a tensor of size 2x3x4\n",
    "t = torch.Tensor(2, 3, 4)\n",
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3xcTvfc7jbr",
    "outputId": "387061b7-c9d2-4c81-e9cc-f93a6f580e72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the size of the tensor\n",
    "t.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDV8gZtQyIYK"
   },
   "source": [
    "# Tensors and tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "okM52HiDyIYL",
    "outputId": "a6d17835-c677-47f1-cbde-7cf9d61da241"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.Tensor(2, 3, 4)\n",
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4BTJlKPWyIYL",
    "outputId": "c459a1cc-1b00-461e-ca30-b6ff2656e8cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pusVGlb5yIYL",
    "outputId": "b98ca6a3-45dc-4378-edee-08977c17cdc6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.1817e+21, 2.0921e-42, 1.4013e-45, 0.0000e+00],\n",
       "         [1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00],\n",
       "         [1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00]],\n",
       "\n",
       "        [[1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00],\n",
       "         [1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00],\n",
       "         [1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00]]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ObX2wLyCyIYL",
    "outputId": "e5aef7a0-1746-4b5c-cf12-d9f0756ccecc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.tensor([1, 2, 3, 4])\n",
    "type(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sll5NbsTyIYL",
    "outputId": "74424481-e4f1-4deb-c83a-dc8d7cd17c72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYJJkp2FyIYL",
    "outputId": "345e6006-4748-481b-d93b-af93028517ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVoVwZLByIYM",
    "outputId": "a5cdf7e6-9ab2-4671-914c-4ac9a257b99e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.tensor([[[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]], [[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]]])\n",
    "type(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y306ZdIXyIYM",
    "outputId": "8b000625-518a-488e-d007-d139a7db7fdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3, 4],\n",
       "         [1, 2, 3, 4],\n",
       "         [1, 2, 3, 4]],\n",
       "\n",
       "        [[1, 2, 3, 4],\n",
       "         [1, 2, 3, 4],\n",
       "         [1, 2, 3, 4]]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwPWsVjqyIYM",
    "outputId": "6c58ea3b-ed30-4e8b-f824-300875124a7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60fsUROsyIYM",
    "outputId": "4e87ed2c-642b-4544-b74f-37e53e289912"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-enxHw_JaeZJ"
   },
   "source": [
    "# Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XXAmpH-Z7jbr",
    "outputId": "376e74b1-f526-46bb-888d-20475283d4de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point in a 24 dimensional space\n",
      "organised in 3 sub-dimensions\n"
     ]
    }
   ],
   "source": [
    "t = torch.Tensor(2, 3, 4)\n",
    "# prints dimensional space and sub-dimensions\n",
    "print(f'point in a {t.numel()} dimensional space')\n",
    "print(f'organised in {t.dim()} sub-dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uC8XADcV7jbr",
    "outputId": "d12e76d0-a3f8-4552-8d64-b66829a48f01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ARC3wOQ7jbr",
    "outputId": "f861d4b5-8643-4cd2-aded-ddb08b2ad474"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3., 5., 9., 6.],\n",
       "         [6., 4., 3., 1.],\n",
       "         [1., 4., 0., 4.]],\n",
       "\n",
       "        [[8., 3., 2., 0.],\n",
       "         [3., 7., 8., 3.],\n",
       "         [1., 5., 8., 2.]]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mind the underscore!\n",
    "# Any operation that mutates a tensor in-place is post-fixed with an _.\n",
    "# For example: x.copy_(y), x.t_(), x.random_(n) will change x.\n",
    "t.random_(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wL2ajaMg7jbs",
    "outputId": "ae1e7d14-4192-49d1-9c12-7d9b3668655c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 5., 9., 6., 6., 4., 3., 1.],\n",
       "        [1., 4., 0., 4., 8., 3., 2., 0.],\n",
       "        [3., 7., 8., 3., 1., 5., 8., 2.]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = t.view(3, 8) # r is the Tensor reshaped to the size 3x8\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ivViPVSG7jbs",
    "outputId": "df99de28-fe2a-45c2-9488-9b16f317c0d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As you can see zero_ would replace r with 0's which was originally filled with integers\n",
    "r.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-7KuYVw17jbs",
    "outputId": "c9734794-f0f4-49c8-c950-3d99a68d176d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uIhTq4nIdqx7",
    "outputId": "6be3ea9e-7329-4f06-c804-c39323585716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 4, 1) (8, 1)\n",
      "torch.Size([2, 3, 4]) torch.Size([3, 8])\n"
     ]
    }
   ],
   "source": [
    "# What are strides. And how are they related to shapes?\n",
    "print(t.stride(), r.stride())\n",
    "print(t.shape, r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uum4hxYkffli",
    "outputId": "a6803271-65e2-4d60-d8e9-231aeb55907c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 5., 8., 1.],\n",
       "         [0., 4., 7., 3.],\n",
       "         [2., 2., 9., 0.]],\n",
       "\n",
       "        [[4., 3., 4., 0.],\n",
       "         [2., 3., 4., 1.],\n",
       "         [5., 6., 9., 6.]]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try that again without doing the operations in place\n",
    "t.random_(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJM4es_M-p2Y",
    "outputId": "f5aab4b7-70cc-49b3-e930-489f85b9f6eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not in place\n",
    "r = t.view(3, 8)\n",
    "r = torch.zeros_like(r)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ivw8wI5U-12D",
    "outputId": "f3d1af00-8062-4e96-c0b9-003758311aed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 5., 8., 1.],\n",
       "         [0., 4., 7., 3.],\n",
       "         [2., 2., 9., 0.]],\n",
       "\n",
       "        [[4., 3., 4., 0.],\n",
       "         [2., 3., 4., 1.],\n",
       "         [5., 6., 9., 6.]]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJdGqvwTdiMs",
    "outputId": "5713c6c6-ffb3-45d1-e42b-2db1f68d03aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 4, 1) (8, 1)\n"
     ]
    }
   ],
   "source": [
    "# What are strides?\n",
    "print(t.stride(), r.stride())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "p6Fcui7u7jbs"
   },
   "outputs": [],
   "source": [
    "# This *is* important\n",
    "s = r.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rm9a0zci7jbt",
    "outputId": "a2463ad9-32de-4710-8b4b-9cd06afa0b79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In-place fill of 1's\n",
    "s.fill_(1)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1K3hywm7jbt",
    "outputId": "a5d21449-5ccd-4918-a6b5-cb881375eb7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because we cloned r, even though we did an in-place operation, this doesn't affect r\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ir1URH3v7jbt"
   },
   "source": [
    "## Vectors (1D Tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dRCvA1R17jbt",
    "outputId": "f6bf895e-dfd7-446c-84ca-457282f2a5f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a 1D tensor of integers 1 to 4\n",
    "v = torch.Tensor([1, 2, 3, 4])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmkSLrIi7jbt",
    "outputId": "7640bf03-1728-4c91-92cf-ca06fb226b92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim: 1, size: 4\n"
     ]
    }
   ],
   "source": [
    "# Print number of dimensions (1D) and size of tensor\n",
    "print(f'dim: {v.dim()}, size: {v.size()[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K03oi68R7jbu",
    "outputId": "73d83678-2cca-484e-ea0c-0f831f0c4a5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 2., 0.])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.Tensor([1, 0, 2, 0])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yGGpVC_b7jbu",
    "outputId": "c7f48242-ae13-40e3-e05d-992aec024e19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 6., 0.])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Element-wise multiplication\n",
    "v * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pq7-Aqs_7jbu",
    "outputId": "5fe812c4-43eb-4f78-fc45-8fd81af68166"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar product: 1*1 + 2*0 + 3*2 + 4*0\n",
    "v @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-oXbFTO7jbu",
    "outputId": "55bd2fcf-129b-40f7-ee8f-4c213108094c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7., 4., 1., 6., 9.])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In-place replacement of random number from 0 to 10\n",
    "x = torch.Tensor(5).random_(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00BUa-L47jbu",
    "outputId": "1f2c8001-c27a-4a2f-d2fb-79415529e7cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first: 7.0, last: 9.0\n"
     ]
    }
   ],
   "source": [
    "print(f'first: {x[0]}, last: {x[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZX_Uy_T17jbu",
    "outputId": "07f19af2-0cf6-4a7b-ce16-fa488d66d28b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 1.])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract sub-Tensor [from:to)\n",
    "x[1:2 + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPNJt5Kt7jbv",
    "outputId": "1e0be6dd-42bb-466b-e479-5b1ec7a0477a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor with integers ranging from 1 to 4 (both included)\n",
    "v = torch.arange(1, 5)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7_nwZPg-7jbv",
    "outputId": "3a73d7a8-e779-41c9-e150-e6e0a8c73db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  4,  9, 16]) tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Square all elements in the tensor\n",
    "print(v.pow(2), v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De7wobZv7jbv"
   },
   "source": [
    "## Matrices (2D Tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuiyP0MK7jbv",
    "outputId": "0e24b697-c974-4993-8417-a52a325893d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 5., 3., 7.],\n",
       "        [4., 2., 1., 9.]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 2x4 tensor\n",
    "m = torch.Tensor([[2, 5, 3, 7],\n",
    "                  [4, 2, 1, 9]])\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xI36U8sv7jbv",
    "outputId": "0eb37690-2005-4e6d-d058-ce82913b1b0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7f91z4dw7jbw",
    "outputId": "28dbd276-d461-45fa-ea07-23405a11a262"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 -- 4 -- torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(m.size(0), m.size(1), m.size(), sep=' -- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vuLnT2z7jbw",
    "outputId": "ed1e81c6-b8a8-4373-83bb-d1c026a0c3e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing row 0, column 2 (0-indexed)\n",
    "m[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YLIC7pG97jbw",
    "outputId": "b3682945-6231-4958-ac3d-0600ddfac6f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing row 0, column 2 (0-indexed)\n",
    "m[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tsutF_zc7jbw",
    "outputId": "7ae29333-d753-439c-8ba8-2aea3eb7a545"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 2.])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing column 1, all rows (returns size 2)\n",
    "m[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLg24cHx7jbw",
    "outputId": "e0055f61-067f-41a4-a001-4456a9dcff3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.],\n",
       "        [2.]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing column 1, all rows (returns size 2x1)\n",
    "m[:, [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8nu79EU7jbx",
    "outputId": "50d8a977-3c73-4a5b-89f3-fd418e4e7891"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 5., 3., 7.]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexes row 0, all columns (returns 1x4)\n",
    "m[[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oYVpTC7l7jbx",
    "outputId": "4f1ea149-66ca-422a-d197-40af8654ff32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensor of numbers from 1 to 5)\n",
    "v = torch.arange(1., 5)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UQd3S3w7jbx",
    "outputId": "46380650-b31a-4224-fe35-21db94deec6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 5., 3., 7.],\n",
       "        [4., 2., 1., 9.]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c10o4XUQ7jbx",
    "outputId": "55fa9b6c-663a-4f4d-d8d0-bbea504fa567"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([49., 47.])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar product\n",
    "m @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fEkCtsZG7jbx",
    "outputId": "8dd167d2-9b6a-4e81-b5ef-82a8ed560eb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(49.)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculated by 1*2 + 2*5 + 3*3 + 4*7\n",
    "m[0, :] @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzxKEjus7jby",
    "outputId": "f83475d1-4a7a-4442-a414-ed1b05871f74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([47.])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculated by\n",
    "m[[1], :] @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mP7c5qI17jby",
    "outputId": "a0c36a00-c7b9-44cd-f823-58c649797cda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4446, 5.7900, 3.7671, 7.2405],\n",
       "        [4.4270, 2.2827, 1.8738, 9.2048]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a random tensor of size 2x4 to m\n",
    "m + torch.rand(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nli9YIb17jby",
    "outputId": "4df6cfa9-7ad6-469f-8b11-b1c91ac520db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0278, 4.7077, 2.0815, 6.9083],\n",
       "        [3.8158, 1.1614, 0.7845, 8.5832]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subtract a random tensor of size 2x4 to m\n",
    "m - torch.rand(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0QPbbr87jby",
    "outputId": "fd2b460d-9e0c-4890-b9d1-186478c49e1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9971, 1.4361, 2.0717, 1.2393],\n",
       "        [0.5686, 0.2749, 0.6850, 6.9691]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply a random tensor of size 2x4 to m\n",
    "m * torch.rand(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wRxwhq3p7jby",
    "outputId": "22a846ea-a556-48de-fd40-45fe99fe6a09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.5549,  8.3898,  3.0347, 30.1543],\n",
       "        [ 8.0203,  3.1156,  1.7936,  9.3968]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide m by a random tensor of size 2x4\n",
    "m / torch.rand(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MFjUamkg7jby",
    "outputId": "85cbfcf7-0085-4bb7-e49a-d942abcac98f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88uWT_-N7jbz",
    "outputId": "cd69f865-4ce5-4a1c-8c9b-0a9b0b72147d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 4.],\n",
       "        [5., 2.],\n",
       "        [3., 1.],\n",
       "        [7., 9.]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transpose tensor m, which is essentially 2x4 to 4x2\n",
    "m.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hfx8uRtl7jbz",
    "outputId": "f48806d2-771e-405b-8abf-07a1f5f0d3aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 4.],\n",
       "        [5., 2.],\n",
       "        [3., 1.],\n",
       "        [7., 9.]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same as\n",
    "m.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3p2bHHeHJewn"
   },
   "source": [
    "## Broadcasting\n",
    "\n",
    "Two tensors are broadcastable if the following rules hold:\n",
    "\n",
    "*   Each tensor has at least one dimension.\n",
    "*   When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "pPyg44mxJeHP"
   },
   "outputs": [],
   "source": [
    "x=torch.empty(5,7,3)\n",
    "y=torch.empty(5,7,3)\n",
    "# x and y are broadcastable since all dimensions are equal\n",
    "\n",
    "x=torch.empty((0,))\n",
    "y=torch.empty(2,2)\n",
    "# x and y are not broadcastable, because x does not have at least 1 dimension\n",
    "\n",
    "x=torch.empty(5,3,4,1)\n",
    "y=torch.empty(  3,1,1)\n",
    "# x and y are broadcastable.\n",
    "# 1st trailing dimension: both have size 1\n",
    "# 2nd trailing dimension: y has size 1\n",
    "# 3rd trailing dimension: x size == y size\n",
    "# 4th trailing dimension: y dimension doesn't exist\n",
    "\n",
    "# but:\n",
    "x=torch.empty(5,2,4,1)\n",
    "y=torch.empty(  3,1,1)\n",
    "# x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MpudbDP9MI4U",
    "outputId": "99e161d6-5474-4472-fcc4-e71529f667ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n"
     ]
    }
   ],
   "source": [
    "# How is the output dimension calculated?\n",
    "x=torch.empty(5,1,4,1)\n",
    "y=torch.empty(3,1,1)\n",
    "print((x+y).size())\n",
    "\n",
    "x=torch.empty(1)\n",
    "y=torch.empty(3,1,7)\n",
    "print((x+y).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpCZhT8U7jbz"
   },
   "source": [
    "## Constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYn7XeYu7jbz",
    "outputId": "ea3ba7ba-0c61-48a9-902c-ed0780d3adbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4., 5., 6., 7., 8.])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensor from 3 to 8\n",
    "torch.arange(3., 8 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XH8X7Dts7jbz",
    "outputId": "6c757fc1-86b7-47c1-a64a-f4c6f24795da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.7000,  2.7000, -0.3000])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensor from 5.7 to -2.1 with step -3\n",
    "torch.arange(5.7, -2.1, -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3B14Dyrn7jbz",
    "outputId": "80df78a3-58f5-4cb9-f751-1ee490640a94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0000, 3.2632, 3.5263, 3.7895, 4.0526, 4.3158, 4.5789, 4.8421, 5.1053,\n",
       "         5.3684, 5.6316, 5.8947, 6.1579, 6.4211, 6.6842, 6.9474, 7.2105, 7.4737,\n",
       "         7.7368, 8.0000]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns a 1D tensor of equally spaced elements between start=3, end=8 and number of elements=20\n",
    "torch.linspace(3, 8, 20).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tb4KEmjU7jb0",
    "outputId": "7f310060-7ffb-4daa-e3f5-717e52b78fbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor filled with 0's\n",
    "torch.zeros(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TrOxrng27jb0",
    "outputId": "a5985151-403f-4ac0-8f12-8592021c4fbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor filled with 1's\n",
    "torch.ones(3, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xt6VUo1A7jb0",
    "outputId": "3d97b3eb-08f7-4da6-d4e7-5449e47887a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor with the diagonal filled with 1\n",
    "torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "oDCnuhOf7jb0"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "id": "81dgFtq67jb0",
    "outputId": "53e98426-964e-4ad0-cb99-3e4d43975519"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Numpy bridge!\n",
    "plt.hist(torch.randn(1000).numpy(), 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "id": "pBhx09297jb1",
    "outputId": "53d2d199-ed4d-4436-8e9b-edc3ddd554b6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABlcAAAMtCAYAAAACLCcVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABJyElEQVR4nO3de5DV9Z3n/xfQ6QYv3Y4aQIpWiM6ojBciatu5WBoZ26QzGydmVxPLoEEtLbQCnVGaGQuNmyoYnZSY4CUpt4K7K+slG82GHiEEI27GNiqGVZnBGjOymGADudCtjDZK9++PKc7aP/HyaRuPwONRdSr29/v5fvt9zh8dZ575nO+w/v7+/gAAAAAAAPCeDK/2AAAAAAAAALsTcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAVqqj1ANfX19WXDhg3Zf//9M2zYsGqPAwAAAAAAVFF/f39efvnljBs3LsOHv/3+lL06rmzYsCGNjY3VHgMAAAAAAPgQefHFFzN+/Pi3Pb9Xx5X9998/yb9/SPX19VWeBgAAAAAAqKaenp40NjZW+sHb2avjyo6vAquvrxdXAAAAAACAJHnXR4l4oD0AAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKPC+4sr8+fMzbNiwzJw5s3Lstddey4wZM3LQQQdlv/32yznnnJONGzcOuG79+vVpbW3NPvvsk9GjR+eqq67KG2+8MWDNww8/nBNOOCF1dXU54ogjsmjRorf8/ltuuSUTJkzIyJEj09TUlMcff/z9vB0AAAAAAIB3Nei48sQTT+R73/tejjvuuAHHZ82alZ/85Ce57777snLlymzYsCFf/OIXK+e3b9+e1tbWbNu2LY8++mjuvPPOLFq0KHPnzq2seeGFF9La2prTTz89q1evzsyZM3PxxRdn2bJllTX33HNP2tracu211+app57K8ccfn5aWlmzatGmwbwkAAAAAAOBdDevv7+8vveiVV17JCSeckFtvvTXf+ta3Mnny5CxYsCDd3d356Ec/msWLF+dLX/pSkmTt2rU5+uij09nZmVNOOSUPPvhgPv/5z2fDhg0ZM2ZMkuT222/P7Nmzs3nz5tTW1mb27Nnp6OjIs88+W/md5513XrZs2ZKlS5cmSZqamnLSSSdl4cKFSZK+vr40NjbmyiuvTHt7+3t6Hz09PWloaEh3d3fq6+tLPwYAAAAAAGAP8l67waB2rsyYMSOtra2ZOnXqgOOrVq3K66+/PuD4UUcdlUMPPTSdnZ1Jks7Ozhx77LGVsJIkLS0t6enpyZo1aypr/v/3bmlpqdxj27ZtWbVq1YA1w4cPz9SpUytrdqa3tzc9PT0DXgAAAAAAACVqSi+4++6789RTT+WJJ554y7murq7U1tbmgAMOGHB8zJgx6erqqqx5c1jZcX7HuXda09PTk1dffTV//OMfs3379p2uWbt27dvOPm/evHzzm998b28UAAAAAABgJ4p2rrz44ov5+te/nrvuuisjR47cVTPtMnPmzEl3d3fl9eKLL1Z7JAAAAAAAYDdTFFdWrVqVTZs25YQTTkhNTU1qamqycuXKfOc730lNTU3GjBmTbdu2ZcuWLQOu27hxY8aOHZskGTt2bDZu3PiW8zvOvdOa+vr6jBo1KgcffHBGjBix0zU77rEzdXV1qa+vH/ACAAAAAAAoURRXzjjjjDzzzDNZvXp15XXiiSfm/PPPr/zzRz7ykaxYsaJyzXPPPZf169enubk5SdLc3JxnnnkmmzZtqqxZvnx56uvrM2nSpMqaN99jx5od96itrc2UKVMGrOnr68uKFSsqawAAAAAAAHaFomeu7L///jnmmGMGHNt3331z0EEHVY5Pnz49bW1tOfDAA1NfX58rr7wyzc3NOeWUU5IkZ555ZiZNmpQLLrggN9xwQ7q6unLNNddkxowZqaurS5JcdtllWbhwYa6++up87Wtfy0MPPZR77703HR0dld/b1taWadOm5cQTT8zJJ5+cBQsWZOvWrbnooove1wcCAAAAAADwToofaP9ubrrppgwfPjznnHNOent709LSkltvvbVyfsSIEVmyZEkuv/zyNDc3Z9999820adNy/fXXV9ZMnDgxHR0dmTVrVm6++eaMHz8+d9xxR1paWiprzj333GzevDlz585NV1dXJk+enKVLl77lIfcAAAAAAABDaVh/f39/tYeolp6enjQ0NKS7u9vzVwAAAAAAYC/3XrtB0TNXAAAAAAAA9nbiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFBBXAAAAAAAACogrAAAAAAAABcQVAAAAAACAAuIKAAAAAABAAXEFAAAAAACgQE21BwAAABhKE9o7BnXduvmtQzwJAACwp7JzBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKOCZKwAAwIfOYJ+bAgAA8EGwcwUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAArUVHsAAABgzzWhvaPaIwAAAAw5O1cAAAAAAAAK2LkCAACQwe+yWTe/dYgnAQAAPuzsXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFBBXAAAAAAAACtRUewAAAODDb0J7R7VHAAAA+NCwcwUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgAI11R4AAABgdzahvWNQ162b3zrEkwAAAB8UO1cAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQIGaag8AAAB8cCa0d1R7BAAAgN2enSsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAVqqj0AAADA3mhCe8egrls3v3WIJwEAAErZuQIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFBBXAAAAAAAACogrAAAAAAAABWqqPQAAAFBuQntHtUcAAADYa9m5AgAAAAAAUEBcAQAAAAAAKFAUV2677bYcd9xxqa+vT319fZqbm/Pggw9Wzp922mkZNmzYgNdll1024B7r169Pa2tr9tlnn4wePTpXXXVV3njjjQFrHn744Zxwwgmpq6vLEUcckUWLFr1llltuuSUTJkzIyJEj09TUlMcff7zkrQAAAAAAAAxKUVwZP3585s+fn1WrVuXJJ5/MZz7zmXzhC1/ImjVrKmsuueSSvPTSS5XXDTfcUDm3ffv2tLa2Ztu2bXn00Udz5513ZtGiRZk7d25lzQsvvJDW1tacfvrpWb16dWbOnJmLL744y5Ytq6y555570tbWlmuvvTZPPfVUjj/++LS0tGTTpk3v57MAAAAAAAB4V8P6+/v7388NDjzwwNx4442ZPn16TjvttEyePDkLFizY6doHH3wwn//857Nhw4aMGTMmSXL77bdn9uzZ2bx5c2prazN79ux0dHTk2WefrVx33nnnZcuWLVm6dGmSpKmpKSeddFIWLlyYJOnr60tjY2OuvPLKtLe3v+fZe3p60tDQkO7u7tTX1w/yEwAAgA+eB9rvvdbNb632CAAAsMd6r91g0M9c2b59e+6+++5s3bo1zc3NleN33XVXDj744BxzzDGZM2dO/u3f/q1yrrOzM8cee2wlrCRJS0tLenp6KrtfOjs7M3Xq1AG/q6WlJZ2dnUmSbdu2ZdWqVQPWDB8+PFOnTq2seTu9vb3p6ekZ8AIAAAAAAChRU3rBM888k+bm5rz22mvZb7/9cv/992fSpElJkq985Ss57LDDMm7cuDz99NOZPXt2nnvuufzoRz9KknR1dQ0IK0kqP3d1db3jmp6enrz66qv54x//mO3bt+90zdq1a99x9nnz5uWb3/xm6VsGAAAAAACoKI4rRx55ZFavXp3u7u788Ic/zLRp07Jy5cpMmjQpl156aWXdsccem0MOOSRnnHFGfv3rX+fwww8f0sEHY86cOWlra6v83NPTk8bGxipOBAAAAAAA7G6K40ptbW2OOOKIJMmUKVPyxBNP5Oabb873vve9t6xtampKkjz//PM5/PDDM3bs2Dz++OMD1mzcuDFJMnbs2Mp/7jj25jX19fUZNWpURowYkREjRux0zY57vJ26urrU1dUVvFsAAAAAAICBBv3MlR36+vrS29u703OrV69OkhxyyCFJkubm5jzzzDPZtGlTZc3y5ctTX19f+Wqx5ubmrFixYsB9li9fXnmuS21tbaZMmTJgTV9fX1asWDHg2S8AAAAAAAC7QtHOlTlz5uSzn/1sDj300Lz88stZvHhxHn744Sxbtiy//vWvs3jx4nzuc5/LQQcdlKeffjqzZs3KqaeemuOOOy5JcuaZZ2bSpEm54IILcsMNN6SrqyvXXHNNZsyYUdlRctlll2XhwoW5+uqr87WvfS0PPfRQ7r333nR0dFTmaGtry7Rp03LiiSfm5JNPzoIFC7J169ZcdNFFQ/jRAAAAAAAAvFVRXNm0aVO++tWv5qWXXkpDQ0OOO+64LFu2LH/xF3+RF198MT/72c8qoaOxsTHnnHNOrrnmmsr1I0aMyJIlS3L55Zenubk5++67b6ZNm5brr7++smbixInp6OjIrFmzcvPNN2f8+PG544470tLSUllz7rnnZvPmzZk7d266uroyefLkLF269C0PuQcAANjTTGjvePdFO7FufusQTwIAAHuvYf39/f3VHqJaenp60tDQkO7u7tTX11d7HAAAeM8G+/9gZ+8lrgAAwLt7r93gfT9zBQAAAAAAYG8irgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAVqqj0AAADszTyYHgAAYPdj5woAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFBBXAAAAAAAACogrAAAAAAAABcQVAAAAAACAAuIKAAAAAABAAXEFAAAAAACggLgCAAAAAABQoKbaAwAAALDrTWjvGNR16+a3DvEkAACw+7NzBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFBBXAAAAAAAACogrAAAAAAAABcQVAAAAAACAAuIKAAAAAABAAXEFAAAAAACggLgCAAAAAABQQFwBAAAAAAAoIK4AAAAAAAAUEFcAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAECBmmoPAAAAe4IJ7R3VHgEAAIAPiJ0rAAAAAAAABcQVAAAAAACAAuIKAAAAAABAAXEFAAAAAACggLgCAAAAAABQQFwBAAAAAAAoIK4AAAAAAAAUEFcAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUKCm2gMAAADw4TWhvWNQ162b3zrEkwAAwIeHnSsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKFBT7QEAAODDZEJ7R7VHAAAA4EPOzhUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFCgKK7cdtttOe6441JfX5/6+vo0NzfnwQcfrJx/7bXXMmPGjBx00EHZb7/9cs4552Tjxo0D7rF+/fq0trZmn332yejRo3PVVVfljTfeGLDm4YcfzgknnJC6urocccQRWbRo0VtmueWWWzJhwoSMHDkyTU1Nefzxx0veCgAAAAAAwKAUxZXx48dn/vz5WbVqVZ588sl85jOfyRe+8IWsWbMmSTJr1qz85Cc/yX333ZeVK1dmw4YN+eIXv1i5fvv27Wltbc22bdvy6KOP5s4778yiRYsyd+7cypoXXnghra2tOf3007N69erMnDkzF198cZYtW1ZZc88996StrS3XXnttnnrqqRx//PFpaWnJpk2b3u/nAQAAAAAA8I6G9ff397+fGxx44IG58cYb86UvfSkf/ehHs3jx4nzpS19KkqxduzZHH310Ojs7c8opp+TBBx/M5z//+WzYsCFjxoxJktx+++2ZPXt2Nm/enNra2syePTsdHR159tlnK7/jvPPOy5YtW7J06dIkSVNTU0466aQsXLgwSdLX15fGxsZceeWVaW9vf8+z9/T0pKGhId3d3amvr38/HwMAAHuICe0d1R4B9gjr5rdWewQAACj2XrvBoJ+5sn379tx9993ZunVrmpubs2rVqrz++uuZOnVqZc1RRx2VQw89NJ2dnUmSzs7OHHvssZWwkiQtLS3p6emp7H7p7OwccI8da3bcY9u2bVm1atWANcOHD8/UqVMra95Ob29venp6BrwAAAAAAABKFMeVZ555Jvvtt1/q6upy2WWX5f7778+kSZPS1dWV2traHHDAAQPWjxkzJl1dXUmSrq6uAWFlx/kd595pTU9PT1599dX87ne/y/bt23e6Zsc93s68efPS0NBQeTU2Npa+fQAAAAAAYC9XHFeOPPLIrF69Or/85S9z+eWXZ9q0afmnf/qnXTHbkJszZ066u7srrxdffLHaIwEAAAAAALuZmtILamtrc8QRRyRJpkyZkieeeCI333xzzj333Gzbti1btmwZsHtl48aNGTt2bJJk7Nixefzxxwfcb+PGjZVzO/5zx7E3r6mvr8+oUaMyYsSIjBgxYqdrdtzj7dTV1aWurq70LQMAAAAAAFQM+pkrO/T19aW3tzdTpkzJRz7ykaxYsaJy7rnnnsv69evT3NycJGlubs4zzzyTTZs2VdYsX7489fX1mTRpUmXNm++xY82Oe9TW1mbKlCkD1vT19WXFihWVNQAAAAAAALtK0c6VOXPm5LOf/WwOPfTQvPzyy1m8eHEefvjhLFu2LA0NDZk+fXra2tpy4IEHpr6+PldeeWWam5tzyimnJEnOPPPMTJo0KRdccEFuuOGGdHV15ZprrsmMGTMqO0ouu+yyLFy4MFdffXW+9rWv5aGHHsq9996bjo6OyhxtbW2ZNm1aTjzxxJx88slZsGBBtm7dmosuumgIPxoAAAAAAIC3KoormzZtyle/+tW89NJLaWhoyHHHHZdly5blL/7iL5IkN910U4YPH55zzjknvb29aWlpya233lq5fsSIEVmyZEkuv/zyNDc3Z9999820adNy/fXXV9ZMnDgxHR0dmTVrVm6++eaMHz8+d9xxR1paWiprzj333GzevDlz585NV1dXJk+enKVLl77lIfcAAAAAAABDbVh/f39/tYeolp6enjQ0NKS7uzv19fXVHgcAgA+BCe0d774IeFfr5rdWewQAACj2XrvB+37mCgAAAAAAwN5EXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFKip9gAAAADseSa0dwzqunXzW4d4EgAAGHp2rgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFBBXAAAAAAAACogrAAAAAAAABcQVAAAAAACAAuIKAAAAAABAgZpqDwAAALvChPaOao8AAADAHsrOFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFBBXAAAAAAAACogrAAAAAAAABcQVAAAAAACAAuIKAAAAAABAAXEFAAAAAACggLgCAAAAAABQQFwBAAAAAAAoIK4AAAAAAAAUEFcAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKBATbUHAAAAgB0mtHcM6rp181uHeBIAAHh7dq4AAAAAAAAUEFcAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFBBXAAAAAAAACogrAAAAAAAABcQVAAAAAACAAuIKAAAAAABAAXEFAAAAAACgQE21BwAAgHcyob2j2iMAAADAAHauAAAAAAAAFBBXAAAAAAAACogrAAAAAAAABcQVAAAAAACAAuIKAAAAAABAAXEFAAAAAACggLgCAAAAAABQQFwBAAAAAAAoIK4AAAAAAAAUEFcAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKBAUVyZN29eTjrppOy///4ZPXp0zj777Dz33HMD1px22mkZNmzYgNdll102YM369evT2tqaffbZJ6NHj85VV12VN954Y8Cahx9+OCeccELq6upyxBFHZNGiRW+Z55ZbbsmECRMycuTINDU15fHHHy95OwAAAAAAAMWK4srKlSszY8aMPPbYY1m+fHlef/31nHnmmdm6deuAdZdcckleeumlyuuGG26onNu+fXtaW1uzbdu2PProo7nzzjuzaNGizJ07t7LmhRdeSGtra04//fSsXr06M2fOzMUXX5xly5ZV1txzzz1pa2vLtddem6eeeirHH398WlpasmnTpsF+FgAAAAAAAO9qWH9/f/9gL968eXNGjx6dlStX5tRTT03y7ztXJk+enAULFuz0mgcffDCf//zns2HDhowZMyZJcvvtt2f27NnZvHlzamtrM3v27HR0dOTZZ5+tXHfeeedly5YtWbp0aZKkqakpJ510UhYuXJgk6evrS2NjY6688sq0t7e/p/l7enrS0NCQ7u7u1NfXD/ZjAABgF5rQ3lHtEYDdwLr5rdUeAQCAPcB77Qbv65kr3d3dSZIDDzxwwPG77rorBx98cI455pjMmTMn//Zv/1Y519nZmWOPPbYSVpKkpaUlPT09WbNmTWXN1KlTB9yzpaUlnZ2dSZJt27Zl1apVA9YMHz48U6dOrazZmd7e3vT09Ax4AQAAAAAAlKgZ7IV9fX2ZOXNmPvnJT+aYY46pHP/KV76Sww47LOPGjcvTTz+d2bNn57nnnsuPfvSjJElXV9eAsJKk8nNXV9c7runp6cmrr76aP/7xj9m+fftO16xdu/ZtZ543b16++c1vDvYtAwAAAAAADD6uzJgxI88++2x+8YtfDDh+6aWXVv752GOPzSGHHJIzzjgjv/71r3P44YcPftIhMGfOnLS1tVV+7unpSWNjYxUnAgAAAAAAdjeDiitXXHFFlixZkkceeSTjx49/x7VNTU1Jkueffz6HH354xo4dm8cff3zAmo0bNyZJxo4dW/nPHcfevKa+vj6jRo3KiBEjMmLEiJ2u2XGPnamrq0tdXd17e5MAAAAAAAA7UfTMlf7+/lxxxRW5//7789BDD2XixInves3q1auTJIccckiSpLm5Oc8880w2bdpUWbN8+fLU19dn0qRJlTUrVqwYcJ/ly5enubk5SVJbW5spU6YMWNPX15cVK1ZU1gAAAAAAAOwKRTtXZsyYkcWLF+fHP/5x9t9//8ozUhoaGjJq1Kj8+te/zuLFi/O5z30uBx10UJ5++unMmjUrp556ao477rgkyZlnnplJkyblggsuyA033JCurq5cc801mTFjRmVXyWWXXZaFCxfm6quvzte+9rU89NBDuffee9PR0VGZpa2tLdOmTcuJJ56Yk08+OQsWLMjWrVtz0UUXDdVnAwAAAAAA8BZFceW2225Lkpx22mkDjv/gBz/IhRdemNra2vzsZz+rhI7Gxsacc845ueaaayprR4wYkSVLluTyyy9Pc3Nz9t1330ybNi3XX399Zc3EiRPT0dGRWbNm5eabb8748eNzxx13pKWlpbLm3HPPzebNmzN37tx0dXVl8uTJWbp06Vsecg8AAAAAADCUhvX39/dXe4hq6enpSUNDQ7q7u1NfX1/tcQAA2IkJ7R3vvghgkNbNb632CAAAfIi8125Q9MwVAAAAAACAvZ24AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFBBXAAAAAAAACogrAAAAAAAABcQVAAAAAACAAuIKAAAAAABAAXEFAAAAAACggLgCAAAAAABQQFwBAAAAAAAoIK4AAAAAAAAUEFcAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKBATbUHAABg7zChvaPaIwAAAMCQsHMFAAAAAACggLgCAAAAAABQQFwBAAAAAAAoIK4AAAAAAAAUEFcAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKFBT7QEAAACgWia0dwz62nXzW4dwEgAAdid2rgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFBBXAAAAAAAACogrAAAAAAAABcQVAAAAAACAAuIKAAAAAABAAXEFAAAAAACggLgCAAAAAABQQFwBAAAAAAAoIK4AAAAAAAAUEFcAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABSoqfYAAADsPia0d1R7BAAAAKg6O1cAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFBBXAAAAAAAACogrAAAAAAAABcQVAAAAAACAAjUli+fNm5cf/ehHWbt2bUaNGpVPfOIT+bu/+7sceeSRlTWvvfZavvGNb+Tuu+9Ob29vWlpacuutt2bMmDGVNevXr8/ll1+en//859lvv/0ybdq0zJs3LzU1/2+chx9+OG1tbVmzZk0aGxtzzTXX5MILLxwwzy233JIbb7wxXV1dOf744/Pd7343J5988iA/CgAAAHjvJrR3DOq6dfNbh3gSAAA+aEU7V1auXJkZM2bksccey/Lly/P666/nzDPPzNatWytrZs2alZ/85Ce57777snLlymzYsCFf/OIXK+e3b9+e1tbWbNu2LY8++mjuvPPOLFq0KHPnzq2seeGFF9La2prTTz89q1evzsyZM3PxxRdn2bJllTX33HNP2tracu211+app57K8ccfn5aWlmzatOn9fB4AAAAAAADvaFh/f3//YC/evHlzRo8enZUrV+bUU09Nd3d3PvrRj2bx4sX50pe+lCRZu3Ztjj766HR2duaUU07Jgw8+mM9//vPZsGFDZTfL7bffntmzZ2fz5s2pra3N7Nmz09HRkWeffbbyu84777xs2bIlS5cuTZI0NTXlpJNOysKFC5MkfX19aWxszJVXXpn29vadztvb25ve3t7Kzz09PWlsbEx3d3fq6+sH+zEAAOw1Bvu/0gbg/7FzBQDgw6unpycNDQ3v2g3e1zNXuru7kyQHHnhgkmTVqlV5/fXXM3Xq1Mqao446Koceemg6OzuTJJ2dnTn22GMHfE1YS0tLenp6smbNmsqaN99jx5od99i2bVtWrVo1YM3w4cMzderUypqdmTdvXhoaGiqvxsbG9/P2AQAAAACAvdCg40pfX19mzpyZT37ykznmmGOSJF1dXamtrc0BBxwwYO2YMWPS1dVVWfPmsLLj/I5z77Smp6cnr776an73u99l+/btO12z4x47M2fOnHR3d1deL774YvkbBwAAAAAA9mpFD7R/sxkzZuTZZ5/NL37xi6GcZ5eqq6tLXV1dtccAAAAAAAB2Y4PauXLFFVdkyZIl+fnPf57x48dXjo8dOzbbtm3Lli1bBqzfuHFjxo4dW1mzcePGt5zfce6d1tTX12fUqFE5+OCDM2LEiJ2u2XEPAAAAAACAXaEorvT39+eKK67I/fffn4ceeigTJ04ccH7KlCn5yEc+khUrVlSOPffcc1m/fn2am5uTJM3NzXnmmWeyadOmyprly5envr4+kyZNqqx58z12rNlxj9ra2kyZMmXAmr6+vqxYsaKyBgAAAAAAYFco+lqwGTNmZPHixfnxj3+c/fffv/J8k4aGhowaNSoNDQ2ZPn162tracuCBB6a+vj5XXnllmpubc8oppyRJzjzzzEyaNCkXXHBBbrjhhnR1deWaa67JjBkzKl/Zddlll2XhwoW5+uqr87WvfS0PPfRQ7r333nR0dFRmaWtry7Rp03LiiSfm5JNPzoIFC7J169ZcdNFFQ/XZAAAAAAAAvEVRXLntttuSJKeddtqA4z/4wQ9y4YUXJkluuummDB8+POecc056e3vT0tKSW2+9tbJ2xIgRWbJkSS6//PI0Nzdn3333zbRp03L99ddX1kycODEdHR2ZNWtWbr755owfPz533HFHWlpaKmvOPffcbN68OXPnzk1XV1cmT56cpUuXvuUh9wAAAAAAAENpWH9/f3+1h6iWnp6eNDQ0pLu7O/X19dUeBwDgQ29Ce8e7LwLgHa2b31rtEQAAeBvvtRsM6oH2AAAAAAAAeytxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFKip9gAAAHzwJrR3VHsEAAAA2G3ZuQIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFBBXAAAAAAAACogrAAAAAAAABcQVAAAAAACAAuIKAAAAAABAAXEFAAAAAACggLgCAAAAAABQQFwBAAAAAAAoIK4AAAAAAAAUEFcAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAArUVHsAAAAA2JtMaO8Y1HXr5rcO8SQAAAyWnSsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKFBT7QEAABi8Ce0d1R4BAAAA9jp2rgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFBBXAAAAAAAACogrAAAAAAAABcQVAAAAAACAAuIKAAAAAABAAXEFAAAAAACggLgCAAAAAABQQFwBAAAAAAAoIK4AAAAAAAAUEFcAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAVqqj0AAAAA8O4mtHcM6rp181uHeBIAAOxcAQAAAAAAKCCuAAAAAAAAFBBXAAAAAAAACogrAAAAAAAABcQVAAAAAACAAsVx5ZFHHslf/uVfZty4cRk2bFgeeOCBAecvvPDCDBs2bMDrrLPOGrDmD3/4Q84///zU19fngAMOyPTp0/PKK68MWPP000/n05/+dEaOHJnGxsbccMMNb5nlvvvuy1FHHZWRI0fm2GOPzT/8wz+Uvh0AAAAAAIAixXFl69atOf7443PLLbe87ZqzzjorL730UuX1P/7H/xhw/vzzz8+aNWuyfPnyLFmyJI888kguvfTSyvmenp6ceeaZOeyww7Jq1arceOONue666/L973+/subRRx/Nl7/85UyfPj2/+tWvcvbZZ+fss8/Os88+W/qWAAAAAAAA3rNh/f39/YO+eNiw3H///Tn77LMrxy688MJs2bLlLTtadvjnf/7nTJo0KU888UROPPHEJMnSpUvzuc99Lr/5zW8ybty43Hbbbfnbv/3bdHV1pba2NknS3t6eBx54IGvXrk2SnHvuudm6dWuWLFlSufcpp5ySyZMn5/bbb9/p7+7t7U1vb2/l556enjQ2Nqa7uzv19fWD/RgAAKpmQntHtUcA4ENu3fzWao8AALDb6OnpSUNDw7t2g13yzJWHH344o0ePzpFHHpnLL788v//97yvnOjs7c8ABB1TCSpJMnTo1w4cPzy9/+cvKmlNPPbUSVpKkpaUlzz33XP74xz9W1kydOnXA721paUlnZ+fbzjVv3rw0NDRUXo2NjUPyfgEAAAAAgL3HkMeVs846K//1v/7XrFixIn/3d3+XlStX5rOf/Wy2b9+eJOnq6sro0aMHXFNTU5MDDzwwXV1dlTVjxowZsGbHz++2Zsf5nZkzZ066u7srrxdffPH9vVkAAAAAAGCvUzPUNzzvvPMq/3zsscfmuOOOy+GHH56HH344Z5xxxlD/uiJ1dXWpq6ur6gwAADvj670AAABg97FLvhbszT72sY/l4IMPzvPPP58kGTt2bDZt2jRgzRtvvJE//OEPGTt2bGXNxo0bB6zZ8fO7rdlxHgAAAAAAYFfY5XHlN7/5TX7/+9/nkEMOSZI0Nzdny5YtWbVqVWXNQw89lL6+vjQ1NVXWPPLII3n99dcra5YvX54jjzwyf/Inf1JZs2LFigG/a/ny5Wlubt7VbwkAAAAAANiLFceVV155JatXr87q1auTJC+88EJWr16d9evX55VXXslVV12Vxx57LOvWrcuKFSvyhS98IUcccURaWlqSJEcffXTOOuusXHLJJXn88cfzj//4j7niiity3nnnZdy4cUmSr3zlK6mtrc306dOzZs2a3HPPPbn55pvT1tZWmePrX/96li5dmm9/+9tZu3Ztrrvuujz55JO54oorhuBjAQAAAAAA2LniuPLkk0/m4x//eD7+8Y8nSdra2vLxj388c+fOzYgRI/L000/nP/yH/5A/+7M/y/Tp0zNlypT87//9vwc86+Suu+7KUUcdlTPOOCOf+9zn8qlPfSrf//73K+cbGhry05/+NC+88EKmTJmSb3zjG5k7d24uvfTSyppPfOITWbx4cb7//e/n+OOPzw9/+MM88MADOeaYY97P5wEAAAAAAPCOhvX39/dXe4hq6enpSUNDQ7q7u1NfX1/tcQCAvZgH2gOwq6yb31rtEQAAdhvvtRvs8meuAAAAAAAA7EnEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFBBXAAAAAAAACogrAAAAAAAABcQVAAAAAACAAuIKAAAAAABAAXEFAAAAAACggLgCAAAAAABQQFwBAAAAAAAoIK4AAAAAAAAUEFcAAAAAAAAK1FR7AAAAAGDXmdDeMajr1s1vHeJJAAD2HHauAAAAAAAAFBBXAAAAAAAACogrAAAAAAAABcQVAAAAAACAAuIKAAAAAABAAXEFAAAAAACggLgCAAAAAABQQFwBAAAAAAAoIK4AAAAAAAAUEFcAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAECBmmoPAACwJ5nQ3lHtEQAAAIBdzM4VAAAAAACAAuIKAAAAAABAAXEFAAAAAACggLgCAAAAAABQQFwBAAAAAAAoIK4AAAAAAAAUEFcAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoEBNtQcAAAAAPnwmtHcM6rp181uHeBIAgA8fO1cAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUKCm2gMAAHwYTWjvqPYIAAAAwIeUnSsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFBBXAAAAAAAACtRUewAAAABgzzGhvWNQ162b3zrEkwAA7DrFO1ceeeSR/OVf/mXGjRuXYcOG5YEHHhhwvr+/P3Pnzs0hhxySUaNGZerUqfmXf/mXAWv+8Ic/5Pzzz099fX0OOOCATJ8+Pa+88sqANU8//XQ+/elPZ+TIkWlsbMwNN9zwllnuu+++HHXUURk5cmSOPfbY/MM//EPp2wEAAAAAAChSHFe2bt2a448/PrfccstOz99www35zne+k9tvvz2//OUvs++++6alpSWvvfZaZc3555+fNWvWZPny5VmyZEkeeeSRXHrppZXzPT09OfPMM3PYYYdl1apVufHGG3Pdddfl+9//fmXNo48+mi9/+cuZPn16fvWrX+Xss8/O2WefnWeffbb0LQEAAAAAALxnw/r7+/sHffGwYbn//vtz9tlnJ/n3XSvjxo3LN77xjfz1X/91kqS7uztjxozJokWLct555+Wf//mfM2nSpDzxxBM58cQTkyRLly7N5z73ufzmN7/JuHHjctttt+Vv//Zv09XVldra2iRJe3t7HnjggaxduzZJcu6552br1q1ZsmRJZZ5TTjklkydPzu23377TeXt7e9Pb21v5uaenJ42Njenu7k59ff1gPwYAYA802K80AQAGx9eCAQAfBj09PWloaHjXbjCkD7R/4YUX0tXVlalTp1aONTQ0pKmpKZ2dnUmSzs7OHHDAAZWwkiRTp07N8OHD88tf/rKy5tRTT62ElSRpaWnJc889lz/+8Y+VNW/+PTvW7Pg9OzNv3rw0NDRUXo2Nje//TQMAAAAAAHuVIY0rXV1dSZIxY8YMOD5mzJjKua6urowePXrA+Zqamhx44IED1uzsHm/+HW+3Zsf5nZkzZ066u7srrxdffLH0LQIAAAAAAHu5mmoP8EGqq6tLXV1dtccAAAAAAAB2Y0O6c2Xs2LFJko0bNw44vnHjxsq5sWPHZtOmTQPOv/HGG/nDH/4wYM3O7vHm3/F2a3acBwAAAAAA2BWGNK5MnDgxY8eOzYoVKyrHenp68stf/jLNzc1Jkubm5mzZsiWrVq2qrHnooYfS19eXpqamyppHHnkkr7/+emXN8uXLc+SRR+ZP/uRPKmve/Ht2rNnxewAAAAAAAHaF4rjyyiuvZPXq1Vm9enWSf3+I/erVq7N+/foMGzYsM2fOzLe+9a38r//1v/LMM8/kq1/9asaNG5ezzz47SXL00UfnrLPOyiWXXJLHH388//iP/5grrrgi5513XsaNG5ck+cpXvpLa2tpMnz49a9asyT333JObb745bW1tlTm+/vWvZ+nSpfn2t7+dtWvX5rrrrsuTTz6ZK6644v1/KgAAAAAAAG+j+JkrTz75ZE4//fTKzzuCx7Rp07Jo0aJcffXV2bp1ay699NJs2bIln/rUp7J06dKMHDmycs1dd92VK664ImeccUaGDx+ec845J9/5zncq5xsaGvLTn/40M2bMyJQpU3LwwQdn7ty5ufTSSytrPvGJT2Tx4sW55ppr8jd/8zf50z/90zzwwAM55phjBvVBAAAAAAAAvBfD+vv7+6s9RLX09PSkoaEh3d3dqa+vr/Y4AMCHyIT2jmqPAAB7lXXzW6s9AgDAe+4GQ/rMFQAAAAAAgD2duAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAArUVHsAAIBdaUJ7R7VHAAAAAPYwdq4AAAAAAAAUEFcAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFCgptoDAAAAAExo7xjUdevmtw7xJAAA787OFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFBBXAAAAAAAACogrAAAAAAAABcQVAAAAAACAAuIKAAAAAABAAXEFAAAAAACggLgCAAAAAABQQFwBAAAAAAAoIK4AAAAAAAAUEFcAAAAAAAAKiCsAAAAAAAAFaqo9AADAezGhvaPaIwAAAAAksXMFAAAAAACgiLgCAAAAAABQQFwBAAAAAAAoIK4AAAAAAAAUEFcAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAVqqj0AAAAAwGBNaO8Y1HXr5rcO8SQAwN7EzhUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAVqqj0AALB3mdDeUe0RAAAAAN4XO1cAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQIGaag8AAAAA8EGb0N4xqOvWzW8d4kkAgN2RnSsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABSoqfYAAMDuaUJ7R7VHAAAAAKgKO1cAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACgw5HHluuuuy7Bhwwa8jjrqqMr51157LTNmzMhBBx2U/fbbL+ecc042btw44B7r169Pa2tr9tlnn4wePTpXXXVV3njjjQFrHn744Zxwwgmpq6vLEUcckUWLFg31WwEAAAAAAHiLXbJz5c///M/z0ksvVV6/+MUvKudmzZqVn/zkJ7nvvvuycuXKbNiwIV/84hcr57dv357W1tZs27Ytjz76aO68884sWrQoc+fOrax54YUX0tramtNPPz2rV6/OzJkzc/HFF2fZsmW74u0AAAAAAABU1OySm9bUZOzYsW853t3dnf/yX/5LFi9enM985jNJkh/84Ac5+uij89hjj+WUU07JT3/60/zTP/1Tfvazn2XMmDGZPHly/vN//s+ZPXt2rrvuutTW1ub222/PxIkT8+1vfztJcvTRR+cXv/hFbrrpprS0tLztXL29vent7a383NPTM8TvHAAAAAAA2NPtkp0r//Iv/5Jx48blYx/7WM4///ysX78+SbJq1aq8/vrrmTp1amXtUUcdlUMPPTSdnZ1Jks7Ozhx77LEZM2ZMZU1LS0t6enqyZs2aypo332PHmh33eDvz5s1LQ0ND5dXY2Dgk7xcAAAAAANh7DHlcaWpqyqJFi7J06dLcdttteeGFF/LpT386L7/8crq6ulJbW5sDDjhgwDVjxoxJV1dXkqSrq2tAWNlxfse5d1rT09OTV1999W1nmzNnTrq7uyuvF1988f2+XQAAAAAAYC8z5F8L9tnPfrbyz8cdd1yamppy2GGH5d57782oUaOG+tcVqaurS11dXVVnAAAAAHZfE9o7BnXduvmtQzwJAFBNu+Rrwd7sgAMOyJ/92Z/l+eefz9ixY7Nt27Zs2bJlwJqNGzdWntEyduzYbNy48S3nd5x7pzX19fVVDzgAAAAAAMCebZfHlVdeeSW//vWvc8ghh2TKlCn5yEc+khUrVlTOP/fcc1m/fn2am5uTJM3NzXnmmWeyadOmyprly5envr4+kyZNqqx58z12rNlxDwAAAAAAgF1lyOPKX//1X2flypVZt25dHn300fzVX/1VRowYkS9/+ctpaGjI9OnT09bWlp///OdZtWpVLrroojQ3N+eUU05Jkpx55pmZNGlSLrjggvyf//N/smzZslxzzTWZMWNG5Su9Lrvssvzrv/5rrr766qxduza33npr7r333syaNWuo3w4AAAAAAMAAQ/7Mld/85jf58pe/nN///vf56Ec/mk996lN57LHH8tGPfjRJctNNN2X48OE555xz0tvbm5aWltx6662V60eMGJElS5bk8ssvT3Nzc/bdd99MmzYt119/fWXNxIkT09HRkVmzZuXmm2/O+PHjc8cdd6SlpWWo3w4AAAAAAMAAw/r7+/urPUS19PT0pKGhId3d3amvr6/2OACwWxnsw1wBAPZGHmgPALuH99oNdvkzVwAAAAAAAPYkQ/61YADA7sUOFAAAAIAydq4AAAAAAAAUEFcAAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKBATbUHAAAAANjTTWjvGPS16+a3DuEkAMBQsHMFAAAAAACggLgCAAAAAABQQFwBAAAAAAAoIK4AAAAAAAAUEFcAAAAAAAAKiCsAAAAAAAAFaqo9AADw/k1o76j2CAAAAAB7DTtXAAAAAAAACogrAAAAAAAABcQVAAAAAACAAuIKAAAAAABAAXEFAAAAAACggLgCAAAAAABQQFwBAAAAAAAoUFPtAQAAAAB4exPaOwZ13br5rUM8CQCwg50rAAAAAAAABcQVAAAAAACAAuIKAAAAAABAAXEFAAAAAACggLgCAAAAAABQoKbaAwAA/8+E9o5qjwAAAADAu7BzBQAAAAAAoIC4AgAAAAAAUEBcAQAAAAAAKCCuAAAAAAAAFBBXAAAAAAAACogrAAAAAAAABWqqPQAAAAAAQ29Ce8egrls3v3WIJwGAPY+dKwAAAAAAAAXEFQAAAAAAgALiCgAAAAAAQAHPXAGAXWCw328NAAAAwIefnSsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAAChQU+0BAAAAAPjwmNDeMajr1s1vHeJJAODDy84VAAAAAACAAuIKAAAAAABAAV8LBgDvYLBfiQAAAADAnsvOFQAAAAAAgALiCgAAAAAAQAFxBQAAAAAAoIC4AgAAAAAAUMAD7QEAAAB43ya0dwzqunXzW4d4EgDY9excAQAAAAAAKGDnCgB7hcH+r+gAAAAA4P/PzhUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFDAA+0BAAAAqJoJ7R2Dum7d/NYhngQA3js7VwAAAAAAAAqIKwAAAAAAAAV8LRgAu5XBfmUAAAAAAAwVO1cAAAAAAAAK2LkCAAAAwG5nsLva181vHeJJANgb2bkCAAAAAABQwM4VAKrCs1MAAAAA2F3ZuQIAAAAAAFBAXAEAAAAAACjga8EAAAAA2GsM9iuK181vHeJJANid2bkCAAAAAABQwM4VAN4XD6YHAAAAYG9j5woAAAAAAEABO1cASGIHCgAAwDvxrBYA3szOFQAAAAAAgAJ2rgAAAADALmLHC8CeSVwB2MP4ei8AAAAA2LXEFQAAAAD4kLHjBeDDTVwB+JCyAwUAAIBS7+f/lhRmAN47cQVgFxJIAAAAAGDPs9vHlVtuuSU33nhjurq6cvzxx+e73/1uTj755GqPBexhRBIAAAD2dL6KDOC9263jyj333JO2trbcfvvtaWpqyoIFC9LS0pLnnnsuo0ePrvZ4wIeQSAIAAABDS5QB9kbD+vv7+6s9xGA1NTXlpJNOysKFC5MkfX19aWxszJVXXpn29va3rO/t7U1vb2/l5+7u7hx66KF58cUXU19f/4HNDXuiY65dVu0RAAAAAIbcs99sqfYIwAeop6cnjY2N2bJlSxoaGt523W67c2Xbtm1ZtWpV5syZUzk2fPjwTJ06NZ2dnTu9Zt68efnmN7/5luONjY27bE4AAAAAYPfVsKDaEwDV8PLLL++ZceV3v/tdtm/fnjFjxgw4PmbMmKxdu3an18yZMydtbW2Vn/v6+vKHP/whBx10UIYNG7ZL54VdYUdFtfsKqBZ/h4Bq8jcIqDZ/h4Bq83cIhl5/f39efvnljBs37h3X7bZxZTDq6upSV1c34NgBBxxQnWFgCNXX1/svUKCq/B0CqsnfIKDa/B0Cqs3fIRha77RjZYfhH8Acu8TBBx+cESNGZOPGjQOOb9y4MWPHjq3SVAAAAAAAwJ5ut40rtbW1mTJlSlasWFE51tfXlxUrVqS5ubmKkwEAAAAAAHuy3fprwdra2jJt2rSceOKJOfnkk7NgwYJs3bo1F110UbVHgw9EXV1drr322rd83R3AB8XfIaCa/A0Cqs3fIaDa/B2C6hnW39/fX+0h3o+FCxfmxhtvTFdXVyZPnpzvfOc7aWpqqvZYAAAAAADAHmq3jysAAAAAAAAfpN32mSsAAAAAAADVIK4AAAAAAAAUEFcAAAAAAAAKiCsAAAAAAAAFxBXYA/X29mby5MkZNmxYVq9eXe1xgL3AunXrMn369EycODGjRo3K4YcfnmuvvTbbtm2r9mjAHuyWW27JhAkTMnLkyDQ1NeXxxx+v9kjAXmLevHk56aSTsv/++2f06NE5++yz89xzz1V7LGAvNX/+/AwbNiwzZ86s9iiwVxFXYA909dVXZ9y4cdUeA9iLrF27Nn19ffne976XNWvW5Kabbsrtt9+ev/mbv6n2aMAe6p577klbW1uuvfbaPPXUUzn++OPT0tKSTZs2VXs0YC+wcuXKzJgxI4899liWL1+e119/PWeeeWa2bt1a7dGAvcwTTzyR733veznuuOOqPQrsdYb19/f3V3sIYOg8+OCDaWtry//8n/8zf/7nf55f/epXmTx5crXHAvZCN954Y2677bb867/+a7VHAfZATU1NOemkk7Jw4cIkSV9fXxobG3PllVemvb29ytMBe5vNmzdn9OjRWblyZU499dRqjwPsJV555ZWccMIJufXWW/Otb30rkydPzoIFC6o9Fuw17FyBPcjGjRtzySWX5L/9t/+WffbZp9rjAHu57u7uHHjggdUeA9gDbdu2LatWrcrUqVMrx4YPH56pU6ems7OzipMBe6vu7u4k8e8+wAdqxowZaW1tHfDvRMAHp6baAwBDo7+/PxdeeGEuu+yynHjiiVm3bl21RwL2Ys8//3y++93v5u///u+rPQqwB/rd736X7du3Z8yYMQOOjxkzJmvXrq3SVMDeqq+vLzNnzswnP/nJHHPMMdUeB9hL3H333XnqqafyxBNPVHsU2GvZuQIfcu3t7Rk2bNg7vtauXZvvfve7efnllzNnzpxqjwzsQd7r36A3++1vf5uzzjor//E//sdccsklVZocAOCDMWPGjDz77LO5++67qz0KsJd48cUX8/Wvfz133XVXRo4cWe1xYK/lmSvwIbd58+b8/ve/f8c1H/vYx/Kf/tN/yk9+8pMMGzascnz79u0ZMWJEzj///Nx55527elRgD/Re/wbV1tYmSTZs2JDTTjstp5xyShYtWpThw/3vOICht23btuyzzz754Q9/mLPPPrtyfNq0admyZUt+/OMfV284YK9yxRVX5Mc//nEeeeSRTJw4sdrjAHuJBx54IH/1V3+VESNGVI5t3749w4YNy/Dhw9Pb2zvgHLBriCuwh1i/fn16enoqP2/YsCEtLS354Q9/mKampowfP76K0wF7g9/+9rc5/fTTM2XKlPz3//7f/cs8sEs1NTXl5JNPzne/+90k//61PIceemiuuOIKD7QHdrn+/v5ceeWVuf/++/Pwww/nT//0T6s9ErAXefnll/N//+//HXDsoosuylFHHZXZs2f7ikL4gHjmCuwhDj300AE/77fffkmSww8/XFgBdrnf/va3Oe2003LYYYfl7//+77N58+bKubFjx1ZxMmBP1dbWlmnTpuXEE0/MySefnAULFmTr1q256KKLqj0asBeYMWNGFi9enB//+MfZf//909XVlSRpaGjIqFGjqjwdsKfbf//93xJQ9t133xx00EHCCnyAxBUA4H1bvnx5nn/++Tz//PNvCbo2yQK7wrnnnpvNmzdn7ty56erqyuTJk7N06dK3POQeYFe47bbbkiSnnXbagOM/+MEPcuGFF37wAwEAHzhfCwYAAAAAAFDAU2YBAAAAAAAKiCsAAAAAAAAFxBUAAAAAAIAC4goAAAAAAEABcQUAAAAAAKCAuAIAAAAAAFBAXAEAAAAAACggrgAAAAAAABQQVwAAAAAAAAqIKwAAAAAAAAXEFQAAAAAAgAL/H99XHEUHbmYFAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(torch.randn(10**6).numpy(), 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyeN3wTV7jb1"
   },
   "source": [
    "## Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "MxMGbcNP7jb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.BFloat16Tensor\n",
       "torch.BoolTensor\n",
       "torch.ByteTensor\n",
       "torch.CharTensor\n",
       "torch.DoubleTensor\n",
       "torch.FloatTensor\n",
       "torch.HalfTensor\n",
       "torch.IntTensor\n",
       "torch.LongTensor\n",
       "torch.ShortTensor\n",
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper to get what kind of tensor types\n",
    "torch.*Tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6pXTH4b7jb1",
    "outputId": "0935e246-ba51-4981-ae28-892b5273494d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 5., 3., 7.],\n",
       "        [4., 2., 1., 9.]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.Tensor([[2, 5, 3, 7],\n",
    "                  [4, 2, 1, 9]])\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3TAG0hTN7jb2",
    "outputId": "9f01ee0a-cf76-4c76-fba0-fd15bf3c9ea2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 5., 3., 7.],\n",
       "        [4., 2., 1., 9.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is basically a 64 bit float tensor\n",
    "m_double = m.double()\n",
    "m_double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hFe9nMZi7jb2",
    "outputId": "55d8232c-32aa-4a36-ced3-090b5b577817"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 5, 3, 7],\n",
       "        [4, 2, 1, 9]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates a tensor of type int8\n",
    "m_byte = m.byte()\n",
    "m_byte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FSVzhX_I7jb2",
    "outputId": "b41d85b8-f518-43cf-8947-2783ed095196"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 5., 3., 7.],\n",
       "       [4., 2., 1., 9.]], dtype=float32)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts tensor to numpy array\n",
    "m_np = m.numpy()\n",
    "m_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHfSv9BB7jb2",
    "outputId": "93494fdc-5e7a-4722-9dcf-4098c0ad8c30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  5.,  3.,  7.],\n",
       "       [ 4.,  2.,  1.,  9.]], dtype=float32)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In-place fill of column 0 and row 0 with value -1\n",
    "m_np[0, 0] = -1\n",
    "m_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICAeMZLU7jb3",
    "outputId": "88b60fca-9940-4100-e11e-b22f54d439cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.,  5.,  3.,  7.],\n",
       "        [ 4.,  2.,  1.,  9.]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CStd3ORV7jb3",
    "outputId": "4688c3b0-2067-4b47-ff83-6270d790c8e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4] tensor([0, 1, 2, 3, 4], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor of integers ranging from 0 to 4\n",
    "import numpy as np\n",
    "n_np = np.arange(5)\n",
    "n = torch.from_numpy(n_np)\n",
    "print(n_np, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3gGi0E-h7jb3",
    "outputId": "03c69de0-891e-41bb-ca9f-3c275d688cc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 4, 6, 8])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In-place multiplication of all elements by 2 for tensor n\n",
    "n.mul_(2)\n",
    "n_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yGGuawb7jb3"
   },
   "source": [
    "## Using the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "id": "V3cSABmzDda2"
   },
   "outputs": [],
   "source": [
    "# If this cell fails you need to change the runtime of your colab notebook to GPU\n",
    "# Go to Runtime -> Change Runtime Type and select GPU\n",
    "assert torch.cuda.is_available(), \"GPU is not enabled\"\n",
    "\n",
    "# use the first gpu available if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rQvVYDMFIAW",
    "outputId": "26e105e6-1e38-4fdc-e72b-edf8c873c23e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor's device: cpu\n",
      "tensor's device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Tensors can be moved between gpu and cpu memory\n",
    "\n",
    "tensor = torch.randn(5, 5) # create a 5x5 matrix filled with random numbers\n",
    "print(f\"tensor's device: {tensor.device}\") # by default tensors are stored in cpu memory (RAM)\n",
    "\n",
    "# Move your tensor to GPU device 0 if there is one (first GPU in the system)\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(device) # tensor.cuda() is an alternative although not recommended\n",
    "print(f\"tensor's device: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "ccZpZ5xzZ8Sy",
    "outputId": "ad98cbc5-59a8-4f21-b3f1-1836f154e567"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[208], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# This throws an exception, since you can't operate on tensors stored in\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# different devices, and the error message is pretty clear about that\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# A common mistake\n",
    "a = torch.randn(5, 2, device=device)\n",
    "b = torch.randn(1, 2)\n",
    "\n",
    "# This throws an exception, since you can't operate on tensors stored in\n",
    "# different devices, and the error message is pretty clear about that\n",
    "c = a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x59Bo0QnEUOU"
   },
   "source": [
    "# Gradient Computation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H25IsxuRKlry",
    "outputId": "1a083fda-e145-4ec7-fb26-c1cddc971a5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of a with respecto to L: tensor([36., 81.])\n"
     ]
    }
   ],
   "source": [
    "# Tensors also track the operations applied on them in order to differentiate them\n",
    "\n",
    "# setting requires_grad to true tells the autograd engine that we want to compute\n",
    "# gradients for this tensor\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "\n",
    "L = 3*a**3\n",
    "L.sum().backward()\n",
    "print(f\"Gradient of a with respecto to L: {a.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnviODjmRdKC"
   },
   "source": [
    "Lets check if the computed gradients are correct:\n",
    "\n",
    "$\\frac{\\partial{L}}{\\partial{a}} = [9 * a_1^2, 9 * a_2^2]$\n",
    "\n",
    "$\\frac{\\partial{L}}{\\partial{a}} = [9 * 2^2, 9 * 3^2]$\n",
    "\n",
    "$\\frac{\\partial{L}}{\\partial{a}} = [36, 81]$\n",
    "\n",
    "As we can see the gradient vector matches the one computed by the autograd engine (no surprise there)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61qpDwtvU_E8",
    "outputId": "b369444a-16c0-46b4-8278-19c2fde4db09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does a require gradients? : False\n",
      "Does b require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "# Notice that the output tensor of an operation will require gradients even\n",
    "# if only a single input tensor has requires_grad=True.\n",
    "\n",
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does a require gradients? : {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does b require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq_YCBR4yIYI"
   },
   "source": [
    "# Autograd with Pytorch: Repeat previous exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsBFjVXQO_jA"
   },
   "source": [
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "Let's repeat with PyTorch one of the gradient calculations what we already did before outselves (with our own auto-differentiation engine):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uN2Jff2gyIYI",
    "outputId": "c1ab9c96-69f9-4a3e-d81f-fa064c3a8f8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = tensor(28., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(4.0, requires_grad = True)  # a = 4\n",
    "b = torch.tensor(3.0, requires_grad = True)  # b = 3\n",
    "c = a + b        # c = 4 + 3\n",
    "\n",
    "res = a * c      # res = a * c = 28\n",
    "\n",
    "print(\"Result =\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hfDIYY5yIYJ"
   },
   "source": [
    "See the warning that the next cell will produce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwt2meXeyIYJ",
    "outputId": "dccb457b-c78a-4a2e-903e-e7912cb4034b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivative of the result with respect to a is: tensor(11.)\n",
      "The derivative of the result with respect to b is: tensor(4.)\n",
      "The derivative of the result with respect to c is: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\musta\\AppData\\Local\\Temp\\ipykernel_22244\\2763502806.py:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(\"The derivative of the result with respect to c is:\", c.grad)\n"
     ]
    }
   ],
   "source": [
    "# Call backprop on the result\n",
    "res.backward()\n",
    "\n",
    "# Now all variables should contain in their \"grad\" the derivative d(res) / d(variable)\n",
    "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
    "print(\"The derivative of the result with respect to b is:\", b.grad)\n",
    "# Also for intermediate results\n",
    "print(\"The derivative of the result with respect to c is:\", c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chEBBK46yIYJ"
   },
   "source": [
    "There are three different things to understand here. PyTorch by default only calculates gradients for the leaf nodes of the computation graph. To do that, all nodes that are necessary for the computation require their gradient to be computed, but the gradient is not maintained, they do not have a `grad` variable. We can ask PyTorch to create a `grad` variable and save the gradient in these intermediate nodes though, if for some reason we are interested in accessing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GB3ybQNhyIYJ",
    "outputId": "a845b39c-354b-4fca-d632-f85bf217780b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XO5g_uAXyIYK",
    "outputId": "2d5e46b3-4265-4bd0-8923-94c6d4f1a41e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.retains_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_xyteFnUyIYK",
    "outputId": "dc90b960-c509-4571-f5e6-1c1bbbca8956"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QxPwYuI3yIYK",
    "outputId": "56bd5726-47da-40ec-8d1c-729aa45764a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requires Grad: True\tRetains Grad: True\tIs leaf: False\n"
     ]
    }
   ],
   "source": [
    "c.retain_grad()\n",
    "\n",
    "print(f\"Requires Grad: {c.requires_grad}\\tRetains Grad: {c.retains_grad}\\tIs leaf: {c.is_leaf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1uxcO-1yIYK",
    "outputId": "b912f6b3-204d-4721-b87e-05cd2a2e8714"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = tensor(28., grad_fn=<MulBackward0>)\n",
      "The derivative of the result with respect to a is: tensor(11.)\n",
      "The derivative of the result with respect to b is: tensor(4.)\n",
      "The derivative of the result with respect to c is: tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(4.0, requires_grad = True)  # a = 4\n",
    "b = torch.tensor(3.0, requires_grad = True)  # b = 3\n",
    "c = a + b        # c = 4 + 3\n",
    "c.retain_grad()\n",
    "\n",
    "res = a * c      # res = a * c = 28\n",
    "\n",
    "print(\"Result =\", res)\n",
    "\n",
    "# Call backprop on the result\n",
    "res.backward()\n",
    "\n",
    "# Now all variables should contain in their \"grad\" the derivative d(res) / d(variable)\n",
    "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
    "print(\"The derivative of the result with respect to b is:\", b.grad)\n",
    "# Also for intermediate results\n",
    "print(\"The derivative of the result with respect to c is:\", c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-_70bB17jb4"
   },
   "source": [
    "## Much more\n",
    "\n",
    "There's definitely much more, but this was the basics about `Tensor`s fun.\n",
    "\n",
    "*Torch* full API can be found [here](https://pytorch.org/docs/stable/index.html).\n",
    "You'll find 100+ `Tensor` operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random numbers, etc are described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Xl6U-RoEtY3"
   },
   "source": [
    "# Homework\n",
    "\n",
    "<font color=\"blue\">**Exercise 1:** The code below simulates a tiny neural network, however it throws an exception. As you build neural networks in PyTorch you will see this exception often. Look at the error message, explain whats happening and make the necessary changes to the code to get an output from this tiny network</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "5aGsG2MMGebg",
    "outputId": "15e0ffad-1f5c-487f-d8bf-80eb47533c76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4490,  0.3699,  0.1368,  0.2974,  0.5646],\n",
      "        [-0.3858,  0.0381,  1.2865,  0.4263, -1.0048],\n",
      "        [-0.5296, -0.0190,  1.4845,  0.4485, -1.2751],\n",
      "        [ 1.3148,  0.7140, -1.0556,  0.1637,  2.1922],\n",
      "        [-1.1953, -0.2836,  2.4014,  0.5513, -2.5266]])\n",
      "torch.Size([5, 5]) torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 5 random normal variables\n",
    "features = torch.randn((1, 5))\n",
    "# True weights for our data, random normal variables again\n",
    "weights = torch.randn_like(features)\n",
    "# and a true bias term\n",
    "bias = torch.randn((1, 1))\n",
    "fts = torch.mm(features.T, weights)\n",
    "print(fts + bias)\n",
    "print(fts.shape, bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoV3PxbRyIYY"
   },
   "source": [
    "<font color=\"blue\">**Exercise 2:** Once you manage to sucessfully run the code above notice how the shape of the tensors ```fts``` and ```bias``` are drastically different, yet they can be added together. Which internal PyTorch mechanism makes this addition happen?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZrd2J_WyIYY"
   },
   "source": [
    "---\n",
    "\n",
    "*Due to broadcasting we can perform the operation of sum with the bias because we fill out the same value to the rest of the shape of the other fts in order to match the operation so it can be performed*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8N-hhzmQZaA"
   },
   "source": [
    "# More Homework\n",
    "\n",
    "<font color=\"blue\">**Exercise 3:** Answer the following questions about the cell below</font>\n",
    "\n",
    "1. Does the value of ```t``` change? Why?\n",
    "2. Does the shape of ```t``` change? Why?\n",
    "3. Explain, in your own words. What is the stride of a tensor, why is it convenient to have them?\n",
    "4.  Pick a mathematical operation like cosine or square root (not those though ). Can you find the correspoding function in the [torch library](https://https://pytorch.org/docs/stable/torch.html#pointwise-ops).\n",
    "5. Apply the function element-wise to ```a```.\n",
    "6. Is there a version of the function that operates in place? Does it return an error? Why? How can it be fixed?\n",
    "7. Run the same function on the GPU. Do you notice any difference in runtime? If not, why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "__SC70eiXYn1",
    "outputId": "353cc339-699b-475f-9228-caee3c436e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  2,  4],\n",
       "        [ 6,  8, 10],\n",
       "        [12, 14, 16]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor(list(range(9)))\n",
    "\n",
    "a = t.view(3, 3)\n",
    "a.mul_(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Yes, t changes with a different view of the tensor's shape with the same data. a and t share the same memory\n",
    "2. No, because we're only viewing it in a different dimension not transforming it.\n",
    "3. The stride is a number we use to determine the number of slots in the tensors we want to jump and skip. For example, the stride on an image is equivalent to jumping n pixels every time. It's convenient because it can potentially improve efficiency because the dimensionality is lowered and it takes less time to compute all operations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  2,  4],\n",
      "        [ 6,  8, 10],\n",
      "        [12, 14, 16]])\n",
      "tensor([[0.0000, 0.9640, 0.9993],\n",
      "        [1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "b = a.tanh()\n",
    "print(a)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[223], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 6\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "# 6\n",
    "b = a.tanh_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.9640, 0.9993],\n",
      "        [1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = a.double()\n",
    "a = a.tanh_()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.7461, 0.7613],\n",
       "        [0.7616, 0.7616, 0.7616],\n",
       "        [0.7616, 0.7616, 0.7616]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a.to(device)\n",
    "a.tanh_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4003, 0.0391, 0.2829,  ..., 0.5751, 0.1137, 0.2640],\n",
      "        [0.3103, 0.4139, 0.4154,  ..., 0.2288, 0.3312, 0.7556],\n",
      "        [0.4399, 0.6844, 0.1148,  ..., 0.7524, 0.3215, 0.3735],\n",
      "        ...,\n",
      "        [0.7107, 0.6916, 0.3005,  ..., 0.7389, 0.0181, 0.6665],\n",
      "        [0.5750, 0.6677, 0.7324,  ..., 0.0701, 0.4248, 0.4865],\n",
      "        [0.3163, 0.4260, 0.7261,  ..., 0.5520, 0.4204, 0.6680]],\n",
      "       device='cuda:0')\n",
      "time taken to run: 0.00038490002043545246\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((40000, 10000))\n",
    "import time\n",
    "device= torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "a = a.to(device)\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "a.tanh_()\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(a)\n",
    "print(\"time taken to run:\", t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
